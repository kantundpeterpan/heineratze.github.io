[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Heiner Atze, PhD, Pharmacist",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\njdaskölfs",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "embm/embm.html#open-source-implementation-of-w1-and-w2-descriptors",
    "href": "embm/embm.html#open-source-implementation-of-w1-and-w2-descriptors",
    "title": "Open-source implemention and large-scale extension of enzyme mechanism-based modeling (EMBM)",
    "section": "Open-source implementation of W1 and W2 descriptors",
    "text": "Open-source implementation of W1 and W2 descriptors"
  },
  {
    "objectID": "embm/embm.html#benchmarking-other-semi-empirical-qm-methods",
    "href": "embm/embm.html#benchmarking-other-semi-empirical-qm-methods",
    "title": "Open-source implemention and large-scale extension of enzyme mechanism-based modeling (EMBM)",
    "section": "Benchmarking other semi-empirical QM methods",
    "text": "Benchmarking other semi-empirical QM methods\n\nxTB\nDFT-TB"
  },
  {
    "objectID": "embm/embm.html#large-scale-application",
    "href": "embm/embm.html#large-scale-application",
    "title": "Open-source implemention and large-scale extension of enzyme mechanism-based modeling (EMBM)",
    "section": "Large-scale application",
    "text": "Large-scale application\n\nDatasets\n\nKinases"
  },
  {
    "objectID": "datacamp/gender/NLP_gender_reviews/notebook.html",
    "href": "datacamp/gender/NLP_gender_reviews/notebook.html",
    "title": "Do students describe professors differently based on gender?",
    "section": "",
    "text": "Note: You can consult the solution of this live training in the file browser as notebook-solution.ipynb\nLanguage plays a crucial role in shaping our perceptions and attitudes towards gender in the workplace, in classrooms, and personal relationships. Studies have shown that gender bias in language can have a significant impact on the way people are perceived and treated.\nFor example, research has found that job advertisements that use masculine-coded language tend to attract more male applicants, while those that use feminine-coded language tend to attract more female applicants. Similarly, gendered language can perpetuate differences in the classroom.\nIn this project, we’ll using scraped student reviews from ratemyprofessors.com to identify differences in language commonly used for male vs. female professors, and explore subtleties in how language in the classroom can be gendered.\nThis excellent tool created by Ben Schmidt allows us to enter the words and phrases that we find in our analysis and explore them in more depth. We’ll do this at the end.\nCatalyst also does some incredible work on decoding gendered language."
  },
  {
    "objectID": "datacamp/gender/NLP_gender_reviews/notebook.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "href": "datacamp/gender/NLP_gender_reviews/notebook.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "title": "Do students describe professors differently based on gender?",
    "section": "3a. What additional package imports are required for data visualization and NLP?",
    "text": "3a. What additional package imports are required for data visualization and NLP?\n::: {#8d6a7a01-887d-4df1-98b6-5a3c95e35519 .cell executionCancelledAt=‘null’ executionTime=‘1184’ jupyter=‘{“outputs_hidden”:false,“source_hidden”:false}’ lastExecutedAt=‘1719766570161’ lastExecutedByKernel=‘f6c43d9d-c556-42d8-9544-e49e4eaa9fbc’ lastScheduledRunId=‘null’ lastSuccessfullyExecutedCode=’import numpy as np # For manipulating matrices during NLP\nimport nltk # Natural language toolkit from nltk.tokenize import word_tokenize # Used for breaking up strings of text (e.g. sentences) into words from nltk.stem.porter import PorterStemmer # Used to return the dictionary base of a word from nltk.tokenize import WhitespaceTokenizer # Used for breaking up strings of text (e.g. sentences) into words based on white space\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # Used to count the occurences of words and phrases from sklearn.feature_extraction import text # Using to extrat features from text"
  },
  {
    "objectID": "datacamp/gender/NLP_gender_reviews/notebook.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "href": "datacamp/gender/NLP_gender_reviews/notebook.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "title": "Do students describe professors differently based on gender?",
    "section": "Task 3d. What are the most important words being used to describe professors in reviews?",
    "text": "Task 3d. What are the most important words being used to describe professors in reviews?\nLet’s write a custom function that tokenizes and lemmatizes our list of words. - Word tokenization: process of splitting text into individual words, called tokens. A common preprocessing step in natural language processing (NLP) so that text can be analyzed and processed more easily. Methods include whitespace tokenization, regular expression-based tokenization, and rule-based tokenization. We’ll be using the word_tokenize tokenizer from nltk, with all its defaults. - Lemmatization: process of reducing words to their base or dictionary form, called the lemma. Also a common pre-processing step in NLP, so that words with a common base form are treated the same way. For example, the lemma of “am” is “be”, of “running” is “run”, and of “mice” is “mouse”.\n\nPorterStemmer().stem(\"she\\'s\")\n\n\"she'\"\n\n\n\nword_tokenize('she\\'s a girl')\n\n['she', \"'s\", 'a', 'girl']\n\n\n\nimport string\n\n\ndef tokenize(text):\n    tk = WhitespaceTokenizer()\n    tokens = tk.tokenize(text)\n    stems = []\n    for item in tokens:\n        stems.append(PorterStemmer().stem(item).strip(string.punctuation))\n    return stems\n\nLet’s import a list of stop words, which are common English words that we will be ignoring in our analysis. sklearn provides a common list of stop words, and we can append additional words to this list. Below, we append pronouns, along with the words “class” and “student”. Feel free to add any additional words you’d like to ignore to this list later on as you try to build upon this analysis!\n\nmy_stop_words = sktext.ENGLISH_STOP_WORDS.union([\"he\",\"she\",\"his\",\"her\",\n                                              \"himself\",\"herself\", \"hers\",\"shes\"\n                                              \"class\",\"student\", 'man', 'woman', 'girl',\n                                                 'guy', 'lady', 'mr', 'mrs', 'ms'])\nmy_stop_words = my_stop_words.union([tokenize(word)[0] for word in my_stop_words])\n\nFor the purpose of analyzing review texts, we want to move from having one row for each professor to one row for each review. Lets do this with .explode() from pandas.\n\ndf_quality = df[(df['review'].apply(len) == df['quality'].apply(len))]\nq = df_quality[['pronouns','review','quality']].explode(['review','quality'], ignore_index=True).dropna()\nq['quality'] = q['quality'].astype(float)\n\n\nq.head(5)\n\n\n\n\n\n\n\n\npronouns\nreview\nquality\n\n\n\n\n0\nF\nGood experience for a class online. It was unc...\n4.0\n\n\n1\nF\nHonestly she didnt teach good at all and she w...\n2.0\n\n\n2\nF\nI think if you go by word for word in the modu...\n1.0\n\n\n3\nF\nTook her online class CSS64. We started buildi...\n1.0\n\n\n4\nF\nTook her for a late start hybrid class (Bus43)...\n3.0\n\n\n\n\n\n\n\nTFIDF vectorization is the process of assigning scores to each review in a document based on how frequently the word occurs, normalized by how frequently the word occurs in the dataset overall.\nWe’ll use TfidfVectorizer() to generate these scores. This will return a matrix, with as many rows as reviews, and as many columns as words in our dataset.\n\nvec = TfidfVectorizer(\n    tokenizer = tokenize,\n    stop_words = list(my_stop_words),\n    ngram_range = (1,4)\n)\nX = vec.fit_transform(q.review)\nfeature_names = vec.get_feature_names_out()\n\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anywh', 'becau', 'el', 'elsewh', 'everywh', 'ind', 'otherwi', 'plea', 'somewh'] not in stop_words.\n  warnings.warn(\n\n\n\nfeature_names.shape\n\n(365516,)\n\n\n\nnp.random.choice(feature_names, size = 10)\n\narray(['tell obviou wast', 'specif test', 'english 70 fail class',\n       'took cours pass/fail', 'class easi understand let',\n       'offic hours realli', 'short tests attend determin', 'a.m',\n       'inform educ', 'suppos like market'], dtype=object)\n\n\nX is a sparse matrix. We’ll now move into filtering X for: - Rows with male professors and reviews of high quality - Rows with female professors and reviews of high quality - Rows with male professors and reviews of low quality - Rows with female professors and reviews of low quality\nWe can explore feature importance in each of these to get a sense of which words and phrases are coming up most often in the data.\n\nm_pos = X[q.pronouns.eq('M') & q.quality.ge(4.5)]\n\n\nm_pos = X[q.pronouns.eq('M') & q.quality.ge(4.5)]\nf_pos = X[q.pronouns.eq('F') & q.quality.ge(4.5)]\nm_neg = X[q.pronouns.eq('M') & q.quality.le(2.5)]\nf_neg = X[q.pronouns.eq('F') & q.quality.le(2.5)]\n\n\nnp.unique(np.array(m_pos[0,:].todense()))\n\narray([0.        , 0.04840186, 0.05048048, 0.05840671, 0.0588915 ,\n       0.06440657, 0.07216968, 0.07743822, 0.08628339, 0.0890904 ,\n       0.09454997, 0.09609708, 0.0982585 , 0.09872669, 0.10101388,\n       0.10570444, 0.11337177, 0.12547472, 0.13281775, 0.13699447,\n       0.146295  , 0.15107613, 0.15781475])\n\n\nLet’s have a look at what language students are using to describe male professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(m_pos.sum(axis = 0)))[0,::-1]\nm_pos_features = feature_names[importance[:300]]\n\nPrint out the 25 most important features\n\nm_pos_features[:25]\n\narray(['comment', 'great', 'class', 'teacher', 'best', 'professor',\n       'prof', 'good', 'help', 'realli', 'make', 'easi', 'love',\n       'great teacher', 'awesom', 'know', 'learn', '', 'work', 'lot',\n       'lectur', 'amaz', 'cours', 'nice', 'test'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively.\n\nimportance = np.argsort(np.asarray(f_pos.sum(axis = 0)))[0,::-1]\nf_pos_features = feature_names[importance[:300]]\nf_pos_features[:25]\n\narray(['comment', 'great', 'class', 'teacher', 'prof', 'help',\n       'professor', 'best', 'good', 'easi', 'realli', 'work', 'nice',\n       'lot', 'love', 'make', 'learn', 'helpful', 'great teacher', '',\n       'amaz', 'great prof', 'cours', 'lectur', 'hard'], dtype=object)\n\n\nIt should be interesting if there are words exclusively used for one gender\n\n#male\nonly_m_pos = ~np.in1d(m_pos_features, f_pos_features)\nm_pos_features[only_m_pos][:25]\n\n/tmp/ipykernel_1311222/3886134522.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_m_pos = ~np.in1d(m_pos_features, f_pos_features)\n\n\narray(['fantast', 'excel teacher', 'topic', 'awesom professor',\n       'excel professor', 'u', 'brilliant', 'realli enjoy', '2', 'old',\n       'amaz professor', \"professor i'v\", \"teacher i'v\", 'mark', 'b',\n       'hot', 'review', 'genuin', 'bore', \"he'll\", 'reason', 'hours',\n       'want learn', 'overal', 'feel'], dtype=object)\n\n\n\n#female\nonly_f_pos = ~np.isin(f_pos_features, m_pos_features)\nf_pos_features[only_f_pos][:25]\n\narray(['extra credit', 'especi', 'spanish', \"she'll\", 'assignments',\n       'realli nice', 'offer', 'help prof', 'class nice', 'succeed',\n       'realli want', 'alot', 'onlin class', 'knowledgable', 'group',\n       'good lectur', 'attention', 'nice help', \"i'd\", 'real world',\n       'easi grade', 'awsom', 'account', 'guid', 'semester'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe male professors negatively.\n\nimportance = np.argsort(np.asarray(m_neg.sum(axis = 0)))[0,::-1]\nm_neg_features = feature_names[importance[:300]]\nm_neg_features[:25]\n\narray(['comment', 'class', 'teach', 'hard', 'test', 'worst', 'professor',\n       'teacher', 'lectur', '', 'time', 'know', 'like', 'grade', \"don't\",\n       \"doesn't\", 'just', 'prof', 'avoid', 'bore', 'question', 'good',\n       'make', 'doe', 'read'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors negatively.\n\nimportance = np.argsort(np.asarray(f_neg.sum(axis = 0)))[0,::-1]\nf_neg_features = feature_names[importance[:300]]\nf_neg_features[:25]\n\narray(['comment', 'class', 'worst', 'grade', 'teacher', 'hard', 'teach',\n       \"don't\", \"doesn't\", 'help', 'like', 'test', '', 'just', 'time',\n       'work', 'professor', 'good', 'doe', 'question', 'make', 'know',\n       'horribl', 'learn', 'unclear'], dtype=object)\n\n\nSame analysis for exclusive words:\n\n#male\nonly_m_neg = ~np.in1d(m_neg_features, f_neg_features)\nm_neg_features[only_m_neg][:25]\n\n/tmp/ipykernel_1311222/728473888.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_m_neg = ~np.in1d(m_neg_features, f_neg_features)\n\n\narray(['speak', 'hard understand', 'great', 'arrog', 'hear', 'costs',\n       'smart', 'hardest', 'taught', 'listen', 'rambl', 'english',\n       'exampl', 'let', 'sit', 'incred', 'wrote', 'knowledg', 'possible',\n       'probabl', 'avoid costs', 'gpa', 'offic', '1', \"can't teach\"],\n      dtype=object)\n\n\n\n#female\nonly_f_neg = ~np.in1d(f_neg_features, m_neg_features)\nf_neg_features[only_f_neg][:25]\n\n/tmp/ipykernel_1311222/1584707764.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_f_neg = ~np.in1d(f_neg_features, m_neg_features)\n\n\narray(['late', 'disorgan', 'gave', 'advis', 'annoy', 'feedback', 'cost',\n       'agre', 'helpful', 'disorganized', 'nice person', 'avoid cost',\n       'unorganized', 'opinion', 'slow', 'quit', 'colleg', 'honestli',\n       'fan', 'papers', 'spanish', 'easi class', 'favorites', 'instead',\n       '5'], dtype=object)"
  },
  {
    "objectID": "datacamp/gender/NLP_gender_reviews/notebook.html#congratulations-on-making-it-to-the-end",
    "href": "datacamp/gender/NLP_gender_reviews/notebook.html#congratulations-on-making-it-to-the-end",
    "title": "Do students describe professors differently based on gender?",
    "section": "Congratulations on making it to the end!",
    "text": "Congratulations on making it to the end!\n\nWhere to from here?\n\nWe can feed these words into Ben Schmidt’s tool to derive insights by field.\nIf you’re interested in learning more about web scraping, take our courses on Web Scraping in Python\nIf you’re intersted in diving in to the world of Natural Language Processing, explore our skill track.\n\n::: {#f592331c-f5d8-4c32-8791-2eee74269815 .cell executionCancelledAt=‘null’ executionTime=‘48’ jupyter=‘{“source_hidden”:true}’ lastExecutedAt=‘1719766585577’ lastExecutedByKernel=‘f6c43d9d-c556-42d8-9544-e49e4eaa9fbc’ lastScheduledRunId=‘null’ lastSuccessfullyExecutedCode=’import os from IPython.display import display, HTML"
  },
  {
    "objectID": "datacamp/gender/NLP_gender_reviews/web_scraping.html",
    "href": "datacamp/gender/NLP_gender_reviews/web_scraping.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "This code won’t run in wowrkspace, but you can download it locally if you are interested in learning about how we obtained the URLs of the professors used in this project.\n\n!pip install selenium!pip install webdriver-manager\n\n\nfrom selenium import webdriverfrom selenium.webdriver.chrome.service import Servicefrom webdriver_manager.chrome import ChromeDriverManagerfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECimport requestsfrom bs4 import BeautifulSoupfrom lxml import etreefrom urllib.request import urlopen\n\n\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))driver.get(\"https://www.ratemyprofessors.com/search/teachers?query=?\")# Wait for initialize, in secondswait = WebDriverWait(driver, 8)wait.until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[5]/div/div/button')))button = driver.find_element('xpath','/html/body/div[5]/div/div/button')button.click()clicks = 0while clicks &lt; 140:    if clicks%10==0:        print(f'Clicked {clicks} times.')    wait = WebDriverWait(driver, 7)    time.sleep(3)    wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[4]/button')))    show_more = driver.find_element('xpath','//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[4]/button')    show_more.click()    clicks += 1cards = driver.find_elements(By.XPATH,'//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[3]/a[*]')driver.quit()profs = [i.get_attribute('href') for i in cards]"
  },
  {
    "objectID": "datacamp/gender/NLP_gender_reviews/notebook-solution.html",
    "href": "datacamp/gender/NLP_gender_reviews/notebook-solution.html",
    "title": "Do students describe professors differently based on gender?",
    "section": "",
    "text": "Language plays a crucial role in shaping our perceptions and attitudes towards gender in the workplace, in classrooms, and personal relationships. Studies have shown that gender bias in language can have a significant impact on the way people are perceived and treated. For example, research has found that job advertisements that use masculine-coded language tend to attract more male applicants, while those that use feminine-coded language tend to attract more female applicants. Similarly, gendered language can perpetuate differences in the classroom.In this project, we’ll using scraped student reviews from ratemyprofessors.com to identify differences in language commonly used for male vs. female professors, and explore subtleties in how language in the classroom can be gendered.This excellent tool created by Ben Schmidt allows us to enter the words and phrases that we find in our analysis and explore them in more depth. We’ll do this at the end.Catalyst also does some incredible work on decoding gendered language."
  },
  {
    "objectID": "datacamp/gender/NLP_gender_reviews/notebook-solution.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "href": "datacamp/gender/NLP_gender_reviews/notebook-solution.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "title": "Do students describe professors differently based on gender?",
    "section": "3a. What additional package imports are required for data visualization and NLP?",
    "text": "3a. What additional package imports are required for data visualization and NLP?\n::: {#8d6a7a01-887d-4df1-98b6-5a3c95e35519 .cell jupyter=‘{“outputs_hidden”:false,“source_hidden”:false}’ executionTime=‘841’ lastSuccessfullyExecutedCode=’import numpy as np # For manipulating matrices during NLP\nimport nltk # Natural language toolkit from nltk.tokenize import word_tokenize # Used for breaking up strings of text (e.g. sentences) into words from nltk.stem.porter import PorterStemmer # Used to return the dictionary base of a word from nltk.tokenize import WhitespaceTokenizer # Used for breaking up strings of text (e.g. sentences) into words based on white space\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # Used to count the occurences of words and phrases from sklearn.feature_extraction import text # Using to extrat features from text"
  },
  {
    "objectID": "datacamp/gender/NLP_gender_reviews/notebook-solution.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "href": "datacamp/gender/NLP_gender_reviews/notebook-solution.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "title": "Do students describe professors differently based on gender?",
    "section": "Task 3d. What are the most important words being used to describe professors in reviews?",
    "text": "Task 3d. What are the most important words being used to describe professors in reviews?\nLet’s write a custom function that tokenizes and lemmatizes our list of words.- Word tokenization: process of splitting text into individual words, called tokens. A common preprocessing step in natural language processing (NLP) so that text can be analyzed and processed more easily. Methods include whitespace tokenization, regular expression-based tokenization, and rule-based tokenization. We’ll be using the word_tokenize tokenizer from nltk, with all its defaults.- Lemmatization: process of reducing words to their base or dictionary form, called the lemma. Also a common pre-processing step in NLP, so that words with a common base form are treated the same way. For example, the lemma of “am” is “be”, of “running” is “run”, and of “mice” is “mouse”.\n\ndef tokenize(text):    tk = WhitespaceTokenizer()    tokens = tk.tokenize(text)    stems = []    for item in tokens:        stems.append(PorterStemmer().stem(item))    return stems    # return tokens\n\nLet’s import a list of stop words, which are common English words that we will be ignoring in our analysis. sklearn provides a common list of stop words, and we can append additional words to this list. Below, we append pronouns, along with the words “class” and “student”.\n\nmy_stop_words = text.ENGLISH_STOP_WORDS.union([\"he\",\"she\",\"his\",\"her\",                                              \"himself\",\"herself\", \"hers\",\"shes\"                                              \"class\",\"student\"])\n\nFor the purpose of analyzing review texts, we want to move from having one row for each professor to one row for each review. Lets do this with .explode() from pandas.\n\ndf_quality = df[(df['review'].apply(len) == df['quality'].apply(len))]q = df_quality[['pronouns','review','quality']].explode(['review','quality'], ignore_index=True).dropna()q['quality'] = q['quality'].astype(float)\n\nTFIDF vectorization is the process of assigning scores to each review in a document based on how frequently the word occurs, normalized by how frequently the word occurs in the dataset overall.We’ll use TfidfVectorizer() to generate these scores. This will return a matrix, with as many rows as reviews, and as many columns as words in our dataset.\n\nvec = TfidfVectorizer(tokenizer=tokenize, stop_words=my_stop_words,                     ngram_range=(1,4))X = vec.fit_transform(q['review'])feature_names = vec.get_feature_names_out()\n\nX is a sparse matrix. We’ll now move into filtering X for:- Male professors only- Female professors only- Rows with male professors and reviews of high quality - Rows with female professors and reviews of high quality - Rows with male professors and reviews of low quality - Rows with female professors and reviews of low quality We can explore feature importance in each of these to get a sense of which words and phrases are coming up most often in the data.\n\nm_pos = X[(q['pronouns']=='M') & (q['quality']&gt;=4.5),:] f_pos = X[(q['pronouns']=='F') & (q['quality']&gt;=4.5),:] m_neg = X[(q['pronouns']=='M') & (q['quality']&lt;2.5),:] f_neg = X[(q['pronouns']=='M') & (q['quality']&lt;2.5),:] \n\nLet’s have a look at what language students are using to describe male professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(m_pos.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'great', 'hi', 'veri', 'class', 'best', 'good',\n       'professor', 'realli', 'teacher', \"he'\", 'wa', 'thi', 'make',\n       'help', 'easi', 'love', 'prof', 'awesom', 'know', 'class.',\n       'learn', 'lectur', 'amaz', 'cours', 'excel', 'just', 'test',\n       'alway', 'prof.', 'lot', 'work', 'nice', 'ha', \"i'v\", 'teach',\n       'teacher.', 'hi class', 'want', 'best professor', 'him.',\n       'professor.', 'like', 'guy', 'class,', 'dr.', 'funni', 'hard',\n       'need', 'read', 'know hi', 'fun', 'clear', 'materi', 'enjoy',\n       'great teacher', 'recommend', 'studi', 'exam', 'care', 'note',\n       'guy.', 'time', 'best prof', 'understand', 'best teacher',\n       'teacher,', 'everi', 'definit', \"don't\", \"you'll\", 'it.', 'onli',\n       'everyth', 'man', 'becaus', 'ani', 'fair', 'took', 'students.',\n       'extrem', 'veri good', 'use', 'had.', 'knowledg', 'explain',\n       'prof!', 'interesting.', 'well.', 'grade', 'veri help', 'question',\n       'fantast', 'math', 'book', 'thi class', 'hi lectur', 'write',\n       'highli', 'think', 'doe', 'attend', 'actual', 'professor,',\n       'thing', 'pretti', 'stuff', 'assign', 'prof,', 'hi students.',\n       'got', 'helpful.', 'befor', 'favorit', 'peopl', 'sure', 'him!',\n       'way', 'care hi', \"it'\", 'long', 'talk', 'absolut', 'super',\n       'guy,', '-', 'passion', 'littl', 'funny,', 'tough', 'hi class.',\n       'cool', '&', 'tri', 'professor!', 'great teacher,', 'wish', 'tell',\n       'person', 'a.', 'good teacher', 'helpful,', 'final', 'make sure',\n       'year', 'expect', 'difficult', 'make class', 'come', 'ask',\n       'love thi', 'subject', 'everyon', 'teacher!', 'man.', 'hi test',\n       'look', 'u', 'material.', 'answer', 'listen', 'sens', 'pay',\n       'class wa', 'great prof.', 'paper', 'offic', 'great professor.',\n       'wa veri', 'worth', 'say', 'far', 'histori', \"you'r\", 'hi stuff',\n       'great teacher.', 'great prof!', 'know hi stuff', 'interesting,',\n       'veri easi', 'easy.', 'bit', 'learn lot', 'veri nice', \"doesn't\",\n       'veri clear', 'classes.', 'thought', \"he' veri\", 'best.', 'smart',\n       'fine.', 'anyon', \"i'v had.\", 'realli enjoy', 'onlin', 'you.',\n       'love hi', 'prepar', 'thi class.', 'stori', 'essay', 'exampl',\n       'entertain', 'attent', 'better', 'course.', 'mani', '2',\n       'homework', 'taken', 'engag', 'bad', 'great prof,', 'notes.',\n       'textbook', 'truli', 'realli know', 'awesome.', 'real',\n       'excel teacher.', \"didn't\", 'inform', 'miss', 'challeng', 'review',\n       'old', 'class!', \"professor i'v\", 'thi guy', 'problem', 'let',\n       'genuin', 'great!', 'too.', 'did', 'work.', 'topic', 'great guy.',\n       \"he'll\", 'best!', 'enjoy hi', 'brilliant', 'veri knowledg',\n       'concept', 'overal', 'bore', \"i'm\", 'dure', \"teacher i'v\",\n       'alway help', 'approach', 'stuff.', 'feel', 'thi professor',\n       'pay attent', 'intellig', 'mark', 'effort', 'midterm', 'great.',\n       'extra', 'tests.', 'veri helpful.', 'instructor', 'text', 'reason',\n       'hi stuff.', 'taught', 'quizz', 'goe', 'lab', 'end', 'teacher!!',\n       'easy,', ':)', 'lot.', 'great professor', 'know hi stuff.',\n       'probabl', 'grade.', 'quit', 'pass', 'exams.', 'fair.', 'kind',\n       'hand', 'incred', 'avail', 'realli know hi', 'highli recommend',\n       \"can't\", 'wonder', 'humor', 'lectures.', 'job', 'him,',\n       'great prof', 'major', 'hour'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(f_pos.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'veri', 'great', 'class', 'help', \"she'\", 'easi',\n       'realli', 'best', 'wa', 'good', 'professor', 'thi', 'teacher',\n       'make', 'love', 'lot', 'prof', 'class.', 'work', 'learn', 'prof.',\n       'nice', 'lectur', 'cours', 'like', 'amaz', 'want', 'excel',\n       'teacher.', 'care', 'know', 'her.', 'alway', 'ha', 'wonder',\n       'just', 'extrem', 'hard', 'awesom', 'recommend', 'dr.', 'need',\n       'test', 'fair', 'time', 'understand', 'professor.', \"don't\",\n       'studi', 'best teacher', 'teach', 'her!', 'read', 'exam', 'grade',\n       'assign', 'class,', 'veri help', 'highli', 'thing',\n       'best professor', 'helpful.', 'materi', 'clear', 'interesting.',\n       'super', \"it'\", \"i'v\", 'onlin', 'great.', 'awesome!', 'enjoy',\n       'extra', 'sure', 'ani', 'thi class', 'definit', 'students.',\n       'pretti', 'becaus', 'question', 'ladi', 'pay', 'everi', 'her,',\n       'talk', 'mrs.', 'veri nice', 'actual', \"you'll\", 'work,',\n       'passion', 'took', 'teacher!', 'veri good', 'homework',\n       \"she' veri\", 'everyth', 'fun', 'class!', 'best prof', 'professor,',\n       'well.', 'come', 'onli', 'better', '-', 'think', 'great prof.',\n       'explain', 'wa veri', 'real', 'highli recommend', 'doe', 'tough',\n       'wish', 'world', 'a.', 'excel prof', 'long', 'use', 'littl',\n       'person', 'material.', 'attent', 'great teacher.', 'sweet',\n       'veri easi', 'peopl', 'prof,', 'helpful,', 'credit', 'had.', '&',\n       'it.', 'anyth', 'great!', 'lab', 'knowledg', 'expect', 'prof!',\n       'problem', 'write', 'professor!', 'teacher,', \"you'r\", 'approach',\n       'look', 'math', 'pass', 'thi cours', 'pay attent', 'discuss',\n       'note', 'you.', 'absolut', 'class wa', 'course.', 'mani', 'ever!',\n       'befor', 'extra credit', 'sens', 'especi', 'taught', 'person.',\n       'got', 'learn lot', 'help.', 'bit', 'work.',\n       'best professor ever!', 'understand.', 'classes.', 'fair.',\n       'lectures.', 'ask', 'great prof', 'nice,', 'grade.', 'tell',\n       'year', 'lady.', 'professor ever!', 'point', 'thought', 'attend',\n       'cool', 'quizz', 'make sure', 'way', 'veri helpful.', 'answer',\n       'students,', 'fine.', \"she'll\", 'easi understand.', 'amazing.',\n       'did', 'end', 'instructor', 'great teacher', 'project', 'challeng',\n       'respect', 'offer', 'great professor!', 'entertain', 'essay',\n       'good lectur', 'veri clear', 'favorit', 'difficult', 'english',\n       'exampl', 'anyon', 'truli', 'great prof!', 'great teacher,', 'goe',\n       'realli want', \"i'm\", 'nice lady.', 'intellig', 'classes!',\n       'awsom', 'concept', 'veri helpful,', 'lot.', 'book', \"doesn't\",\n       'group', 'lectures,', 'them.', 'listen', 'kind', 'smart', '3',\n       'funni', \"i'd\", 'easy.', 'veri fair', 'great professor,', 'tri',\n       \"she' realli\", 'make class', 'open', 'guid', 'recommend her.',\n       'inspir', 'spanish', 'account', 'probabl', 'offic', 'easi grade!',\n       'paper', 'school', 'woman', 'great prof,', 'great teacher!',\n       'questions.', 'taken', 'help prof', 'real world', 'love her!',\n       'stori', 'good professor.', 'let', \"isn't\", 'effort', \"prof i'v\",\n       'worth', 'alot', 'avail', 'funny.', 'subject', 'book.', 'hour',\n       'job', 'too.', 'yummer', 'best!!', 'ã\\x82â', 'knowledgable.',\n       'research', 'say', 'clearli', 'engag', 'realli know', 'helpful!',\n       'requir', 'prepar', 'demand', 'exams.', 'midterm', 'thank', 'far',\n       'great professor.', 'easy,'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe male professors negatively.\n\nimportance = np.argsort(np.asarray(m_neg.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'hi', 'thi', 'class', 'veri', 'wa', 'teach', 'worst',\n       'test', 'lectur', \"doesn't\", 'class.', 'like', 'hard', 'just',\n       \"don't\", 'professor', 'doe', 'know', 'grade', 'make', 'avoid',\n       'bore', 'teacher', \"he'\", 'question', 'onli', 'terribl', 'good',\n       'read', 'horribl', 'ha', 'learn', 'cours', 'prof', 'time', 'exam',\n       'understand', 'thi class', 'talk', 'becaus', 'help', 'realli',\n       'guy', 'him.', 'explain', 'bad', 'say', 'ask', 'ani', 'hi class',\n       \"can't\", 'thing', 'think', \"didn't\", 'way', 'book', 'thi guy',\n       'noth', 'need', 'materi', 'answer', 'wast', 'hi lectur', 'extrem',\n       'work', 'everi', 'want', 'expect', 'nice', 'teacher.', 'class,',\n       'assign', 'worst professor', 'all.', 'studi', 'professor.', 'did',\n       'note', 'anyth', 'use', \"i'v\", 'difficult', 'final', 'unclear',\n       'tell', 'drop', 'hi test', 'complet', 'worst prof', 'confus',\n       'dont', 'man', 'tri', 'absolut', 'got', 'time.', 'everyth', '-',\n       'easi', 'had.', 'rude', 'it.', 'write', 'fail', 'stay',\n       'worst teacher', '3', 'point', 'math', 'hard.', 'class wa', 'lot',\n       'mark', \"it'\", 'day', 'took', 'students.', 'clear', 'anoth',\n       'homework', 'tests.', 'useless', 'recommend', 'awful.', 'peopl',\n       'goe', 'littl', 'entir', 'unless', 'word', 'hi class.', 'someon',\n       'actual', 'come', 'textbook', 'hour', 'boring.', 'thi professor',\n       'lectures.', 'problem', 'dure', 'attend', 'better', 'half', 'went',\n       'veri hard', 'aw', 'care', 'pass', 'doesnt', 'prepar', 'everyon',\n       'person', 'paper', 'course.', 'ever.', 'look', '2', 'whi', 'mean',\n       'feel', 'year', 'arrog', 'midterm', 'prof.', 'ask question',\n       'thi class.', 'u', 'alway', \"i'm\", 'pretti', \"you'r\", 'hardest',\n       'speak', 'hate', 'imposs', 'old', 'befor', '&', 'end', 'thi cours',\n       'poor', 'subject', 'semest', 'said', 'told', 'guy,', 'someth',\n       'test.', 'basic', 'grade.', 'project', 'thi man', \"isn't\", 'mani',\n       'onlin', 'rambl', 'listen', 'idea', 'sit', 'base', 'differ',\n       'tough', 'incred', 'taught', 'essay', 'him,', 'away', 'hear',\n       'major', 'you.', \"won't\", 'terrible.', 'school', 'total', 'random',\n       'refus', 'concept', 'teach.', 'high', 'sure', 'exampl', 'slide',\n       'probabl', \"you'll\", 'anything.', 'veri difficult', 'week',\n       'hi teach', 'averag', 'questions.', 'follow', 'smart', 'save',\n       \"i'v had.\", 'possibl', 'avoid thi', 'right', 'life', 'wrote',\n       'far', 'boring,', 'bore lectur', 'topic', 'offic', 'knowledg',\n       'long', 'present', 'costs.', 'hi note', 'dr.', \"he'll\",\n       'answer question', 'believ', 'worth', 'pleas', \"don't know\", 'els',\n       'lab', '10', 'wrong.', 'run', 'harder', 'material.', 'wast time.',\n       'teacher,', \"doesn't explain\", 'let', 'nice guy', 'again.',\n       'veri bore', 'definit', 'quizz', 'chang', 'email', 'teaching.',\n       'them.', \"can't teach\", 'hi exam', 'minut', 'lecture.',\n       'thi teacher', 'pick', 'veri unclear', 'taken', 'avoid costs.',\n       'hard,', 'theori', 'book.', 'life.', \"wasn't\", 'suck', 'guy.',\n       ':(', \"worst professor i'v\", 'text', 'prof ever.', '....', 'group'],\n      dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively.\n\nimportance = np.argsort(np.asarray(f_neg.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'hi', 'thi', 'class', 'veri', 'wa', 'teach', 'worst',\n       'test', 'lectur', \"doesn't\", 'class.', 'like', 'hard', 'just',\n       \"don't\", 'professor', 'doe', 'know', 'grade', 'make', 'avoid',\n       'bore', 'teacher', \"he'\", 'question', 'onli', 'terribl', 'good',\n       'read', 'horribl', 'ha', 'learn', 'cours', 'prof', 'time', 'exam',\n       'understand', 'thi class', 'talk', 'becaus', 'help', 'realli',\n       'guy', 'him.', 'explain', 'bad', 'say', 'ask', 'ani', 'hi class',\n       \"can't\", 'thing', 'think', \"didn't\", 'way', 'book', 'thi guy',\n       'noth', 'need', 'materi', 'answer', 'wast', 'hi lectur', 'extrem',\n       'work', 'everi', 'want', 'expect', 'nice', 'teacher.', 'class,',\n       'assign', 'worst professor', 'all.', 'studi', 'professor.', 'did',\n       'note', 'anyth', 'use', \"i'v\", 'difficult', 'final', 'unclear',\n       'tell', 'drop', 'hi test', 'complet', 'worst prof', 'confus',\n       'dont', 'man', 'tri', 'absolut', 'got', 'time.', 'everyth', '-',\n       'easi', 'had.', 'rude', 'it.', 'write', 'fail', 'stay',\n       'worst teacher', '3', 'point', 'math', 'hard.', 'class wa', 'lot',\n       'mark', \"it'\", 'day', 'took', 'students.', 'clear', 'anoth',\n       'homework', 'tests.', 'useless', 'recommend', 'awful.', 'peopl',\n       'goe', 'littl', 'entir', 'unless', 'word', 'hi class.', 'someon',\n       'actual', 'come', 'textbook', 'hour', 'boring.', 'thi professor',\n       'lectures.', 'problem', 'dure', 'attend', 'better', 'half', 'went',\n       'veri hard', 'aw', 'care', 'pass', 'doesnt', 'prepar', 'everyon',\n       'person', 'paper', 'course.', 'ever.', 'look', '2', 'whi', 'mean',\n       'feel', 'year', 'arrog', 'midterm', 'prof.', 'ask question',\n       'thi class.', 'u', 'alway', \"i'm\", 'pretti', \"you'r\", 'hardest',\n       'speak', 'hate', 'imposs', 'old', 'befor', '&', 'end', 'thi cours',\n       'poor', 'subject', 'semest', 'said', 'told', 'guy,', 'someth',\n       'test.', 'basic', 'grade.', 'project', 'thi man', \"isn't\", 'mani',\n       'onlin', 'rambl', 'listen', 'idea', 'sit', 'base', 'differ',\n       'tough', 'incred', 'taught', 'essay', 'him,', 'away', 'hear',\n       'major', 'you.', \"won't\", 'terrible.', 'school', 'total', 'random',\n       'refus', 'concept', 'teach.', 'high', 'sure', 'exampl', 'slide',\n       'probabl', \"you'll\", 'anything.', 'veri difficult', 'week',\n       'hi teach', 'averag', 'questions.', 'follow', 'smart', 'save',\n       \"i'v had.\", 'possibl', 'avoid thi', 'right', 'life', 'wrote',\n       'far', 'boring,', 'bore lectur', 'topic', 'offic', 'knowledg',\n       'long', 'present', 'costs.', 'hi note', 'dr.', \"he'll\",\n       'answer question', 'believ', 'worth', 'pleas', \"don't know\", 'els',\n       'lab', '10', 'wrong.', 'run', 'harder', 'material.', 'wast time.',\n       'teacher,', \"doesn't explain\", 'let', 'nice guy', 'again.',\n       'veri bore', 'definit', 'quizz', 'chang', 'email', 'teaching.',\n       'them.', \"can't teach\", 'hi exam', 'minut', 'lecture.',\n       'thi teacher', 'pick', 'veri unclear', 'taken', 'avoid costs.',\n       'hard,', 'theori', 'book.', 'life.', \"wasn't\", 'suck', 'guy.',\n       ':(', \"worst professor i'v\", 'text', 'prof ever.', '....', 'group'],\n      dtype=object)"
  },
  {
    "objectID": "datacamp/gender/NLP_gender_reviews/notebook-solution.html#congratulations-on-making-it-to-the-end-where-to-from-here--we-can-feed-these-words-into-ben-schmidts-tool-to-derive-insights-by-field.--if-youre-interested-in-learning-more-about-web-scraping-take-our-courses-on-web-scraping-in-python--if-youre-intersted-in-diving-in-to-the-world-of-natural-language-processing-explore-our-skill-track.",
    "href": "datacamp/gender/NLP_gender_reviews/notebook-solution.html#congratulations-on-making-it-to-the-end-where-to-from-here--we-can-feed-these-words-into-ben-schmidts-tool-to-derive-insights-by-field.--if-youre-interested-in-learning-more-about-web-scraping-take-our-courses-on-web-scraping-in-python--if-youre-intersted-in-diving-in-to-the-world-of-natural-language-processing-explore-our-skill-track.",
    "title": "Do students describe professors differently based on gender?",
    "section": "Congratulations on making it to the end! ### Where to from here?- We can feed these words into Ben Schmidt’s tool to derive insights by field.- If you’re interested in learning more about web scraping, take our courses on Web Scraping in Python- If you’re intersted in diving in to the world of Natural Language Processing, explore our skill track.",
    "text": "Congratulations on making it to the end! ### Where to from here?- We can feed these words into Ben Schmidt’s tool to derive insights by field.- If you’re interested in learning more about web scraping, take our courses on Web Scraping in Python- If you’re intersted in diving in to the world of Natural Language Processing, explore our skill track."
  },
  {
    "objectID": "guided.html",
    "href": "guided.html",
    "title": "Datacamp guided projects and code-alongs",
    "section": "",
    "text": "Can I give some explanation here =\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo students describe professors differently based on gender?\n\n\n\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nEMBM\n\n\nfunny but useless\n\n\n\nHeiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nOpen-source implemention and large-scale extension of enzyme mechanism-based modeling (EMBM)\n\n\nWe do not really need this either, but it is also fun.\n\n\n\nHeiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\nTrump vs. Trudeau: Tweet classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "embm/todo.html",
    "href": "embm/todo.html",
    "title": "EMBM",
    "section": "",
    "text": "Structural information for ligand used to validate the open-source implementation of EMBM were extracted from the CHEMBL database using the PUBMED ID of the respekkctive publication.\n\n\n\nThe obtained flat files were loaded into a pandas dataframe and processed as follows: Starting from the smiles representation, the OpenBabel cheminformatics toolkit was used to assign protonation states of functional groups at pH = 7.4.\nThe implementation of Reaction SMARTS in RDKit was used to construct ligand structure preparation pipelines. For the validation of the open-source implementation, molecules were truncated at the warhead such that substituents where varied only on one side of the warhead while the rest of molecule was replaced by a methyl placeholder. I anticipated that this method is applicable only to terminal warheads or modeling of relative energy differences in a congeneric series. For validation purposes, I kept the ligand preparation procedure as close as possible to the published procedure. Aiming for a more generally applicable approach was devised during which the ligand structure is truncated around the warhead. First, all atom within a distance of X(3 to 5) bonds of the electrophilic center of the warhead are selected. If these atoms are ring members, the rings are added to the selection in their entirety along with all functional groups they might bear. Second, within the fragmnent extracted in the first, a scan for chains of rotatable bonds is performed. All chains exceeding 2 sequential rotatable bonds are trimmed in order to reduce conformational complexity facilitating conformational searches and SQM-level geometry optimizations.\nand to construct representations for the anionic and neutral covalent reaction products with the two nucleophiles. Manual definition and validation of the warhead SMARTS pattern was necessary to ensure correct reaction mapping.\n\n\n\n\n\n\n\n\n\n!!! Train and test set molecules were tested against different targets!!!\n\nRefs\n\nTRAIN Constanzo, 2005, J Med Chem, PMID 15771442\nExclusion of compounds 50 and 54 without any comment\n\ncompoundkey 51,52,53,55,56,57,58,59,60,61,62,63,64,65,66,67,68,4,1b\ntargetid CHEMBL3769\nassayid CHEMBL828215\n\nDONE TEST Bachand, 2001, Bioorg Med Chem Lett, PMID 11212093 [1/1]\ncompounds 2e and 2k excluded due to missing data\n\ncompoundkey 2b,2c,2d,2f,2h,2i,2j\ntargetid CHEMBL209 (HUMAN trypsin)\nassayid CHEMBL817084\n\n\n\n\n\n\n\nRefs\n\nTRAIN Constanzo, 2005, J Med Chem, PMID 15771442\n\ncompoundkeys 50,52,53,54,55,58,59,60,63,64,65,66,67,68,4,1b\ntargetid CHEMBL3769\nassayid CHEMBL828215\n\nTEST Constanzo, 2005, J Med Chem, PMID 15771442\n\ncompoundkeys 51,56,57,61,62\ntargetid CHEBML3769\nassayid CHEMBL828215\n\n\n\n\n\n\n\nRefs\n\nTRAIN Edwards, 1995, J Med Chem, PMID 7837243\n\ncompoundkeys all but 1d and 1m"
  },
  {
    "objectID": "embm/todo.html#methods",
    "href": "embm/todo.html#methods",
    "title": "EMBM",
    "section": "",
    "text": "Structural information for ligand used to validate the open-source implementation of EMBM were extracted from the CHEMBL database using the PUBMED ID of the respekkctive publication.\n\n\n\nThe obtained flat files were loaded into a pandas dataframe and processed as follows: Starting from the smiles representation, the OpenBabel cheminformatics toolkit was used to assign protonation states of functional groups at pH = 7.4.\nThe implementation of Reaction SMARTS in RDKit was used to construct ligand structure preparation pipelines. For the validation of the open-source implementation, molecules were truncated at the warhead such that substituents where varied only on one side of the warhead while the rest of molecule was replaced by a methyl placeholder. I anticipated that this method is applicable only to terminal warheads or modeling of relative energy differences in a congeneric series. For validation purposes, I kept the ligand preparation procedure as close as possible to the published procedure. Aiming for a more generally applicable approach was devised during which the ligand structure is truncated around the warhead. First, all atom within a distance of X(3 to 5) bonds of the electrophilic center of the warhead are selected. If these atoms are ring members, the rings are added to the selection in their entirety along with all functional groups they might bear. Second, within the fragmnent extracted in the first, a scan for chains of rotatable bonds is performed. All chains exceeding 2 sequential rotatable bonds are trimmed in order to reduce conformational complexity facilitating conformational searches and SQM-level geometry optimizations.\nand to construct representations for the anionic and neutral covalent reaction products with the two nucleophiles. Manual definition and validation of the warhead SMARTS pattern was necessary to ensure correct reaction mapping."
  },
  {
    "objectID": "embm/todo.html#open-source-validation",
    "href": "embm/todo.html#open-source-validation",
    "title": "EMBM",
    "section": "",
    "text": "!!! Train and test set molecules were tested against different targets!!!\n\nRefs\n\nTRAIN Constanzo, 2005, J Med Chem, PMID 15771442\nExclusion of compounds 50 and 54 without any comment\n\ncompoundkey 51,52,53,55,56,57,58,59,60,61,62,63,64,65,66,67,68,4,1b\ntargetid CHEMBL3769\nassayid CHEMBL828215\n\nDONE TEST Bachand, 2001, Bioorg Med Chem Lett, PMID 11212093 [1/1]\ncompounds 2e and 2k excluded due to missing data\n\ncompoundkey 2b,2c,2d,2f,2h,2i,2j\ntargetid CHEMBL209 (HUMAN trypsin)\nassayid CHEMBL817084\n\n\n\n\n\n\n\nRefs\n\nTRAIN Constanzo, 2005, J Med Chem, PMID 15771442\n\ncompoundkeys 50,52,53,54,55,58,59,60,63,64,65,66,67,68,4,1b\ntargetid CHEMBL3769\nassayid CHEMBL828215\n\nTEST Constanzo, 2005, J Med Chem, PMID 15771442\n\ncompoundkeys 51,56,57,61,62\ntargetid CHEBML3769\nassayid CHEMBL828215\n\n\n\n\n\n\n\nRefs\n\nTRAIN Edwards, 1995, J Med Chem, PMID 7837243\n\ncompoundkeys all but 1d and 1m"
  },
  {
    "objectID": "datacamp/gender/NLP_gender_reviews/notebook.html#3a-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "href": "datacamp/gender/NLP_gender_reviews/notebook.html#3a-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "title": "Do students describe professors differently based on gender?",
    "section": "3a. What additional package imports are required for data visualization and NLP?",
    "text": "3a. What additional package imports are required for data visualization and NLP?"
  },
  {
    "objectID": "datacamp/gender/NLP_gender_reviews/notebook.html#task-3d-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "href": "datacamp/gender/NLP_gender_reviews/notebook.html#task-3d-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "title": "Do students describe professors differently based on gender?",
    "section": "Task 3d. What are the most important words being used to describe professors in reviews?",
    "text": "Task 3d. What are the most important words being used to describe professors in reviews?"
  },
  {
    "objectID": "ttt/trump_trudeau_tweets.html",
    "href": "ttt/trump_trudeau_tweets.html",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "",
    "text": "So you think you can classify text? How about tweets? In this notebook, we’ll take a dive into the world of social media text classification by investigating how to properly classify tweets from two prominent North American politicians: Donald Trump and Justin Trudeau.\n\n\n\n\n\nPhoto Credit: Executive Office of the President of the United States\n\n\nTweets pose specific problems to NLP, including the fact they are shorter texts. There are also plenty of platform-specific conventions to give you hassles: mentions, #hashtags, emoji, links and short-hand phrases (ikr?). Can we overcome those challenges and build a useful classifier for these two tweeters? Yes! Let’s get started.\n\n\nTo begin, we will import all the tools we need from scikit-learn. We will need to properly vectorize our data (CountVectorizer and TfidfVectorizer). And we will also want to import some models, including MultinomialNB from the naive_bayes module, LinearSVC from the svm module and PassiveAggressiveClassifier from the linear_model module. Finally, we’ll need sklearn.metrics and train_test_split and GridSearchCV from the model_selection module to evaluate and optimize our model.\n\n\n# Set seed for reproducibility\nimport random; random.seed(53)\n\n# Import all we need from sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics"
  },
  {
    "objectID": "ttt/trump_trudeau_tweets.html#transforming-our-collected-data",
    "href": "ttt/trump_trudeau_tweets.html#transforming-our-collected-data",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "2. Transforming our collected data",
    "text": "2. Transforming our collected data\n\nTo begin, let’s start with a corpus of tweets which were collected in November 2017. They are available in CSV format. We’ll use a Pandas DataFrame to help import the data and pass it to scikit-learn for further processing.\n\n\nSince the data has been collected via the Twitter API and not split into test and training sets, we’ll need to do this. Let’s use train_test_split() with random_state=53 and a test size of 0.33, just as we did in the DataCamp course. This will ensure we have enough test data and we’ll get the same results no matter where or when we run this code.\n\n\nimport pandas as pd\n\n# Load data\ntweet_df = pd.read_csv('datasets/tweets.csv')\n\n# Create target\ny = tweet_df['author']\nX = tweet_df.drop('author', axis = 1)\n\n# Split training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 53, test_size = 0.33)"
  },
  {
    "objectID": "ttt/trump_trudeau_tweets.html#vectorize-the-tweets",
    "href": "ttt/trump_trudeau_tweets.html#vectorize-the-tweets",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "3. Vectorize the tweets",
    "text": "3. Vectorize the tweets\n\nWe have the training and testing data all set up, but we need to create vectorized representations of the tweets in order to apply machine learning.\n\n\nTo do so, we will utilize the CountVectorizer and TfidfVectorizer classes which we will first need to fit to the data.\n\n\nOnce this is complete, we can start modeling with the new vectorized tweets!\n\n\n# Initialize count vectorizer\ncount_vectorizer = CountVectorizer(max_df = 0.9, min_df = 0.05, stop_words = 'english')\ncount_vectorizer.fit(X_train['status'])\n\n# Create count train and test variables\ncount_train = count_vectorizer.transform(X_train['status'])\ncount_test = count_vectorizer.transform(X_test['status'])\n\n# Initialize tfidf vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.9, min_df = 0.05)\\\n                    .fit(X_train['status'])\n\n# Create tfidf train and test variables\ntfidf_train = tfidf_vectorizer.transform(X_train['status'])\ntfidf_test = tfidf_vectorizer.transform(X_test['status'])"
  },
  {
    "objectID": "ttt/trump_trudeau_tweets.html#training-a-multinomial-naive-bayes-model",
    "href": "ttt/trump_trudeau_tweets.html#training-a-multinomial-naive-bayes-model",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "4. Training a multinomial naive Bayes model",
    "text": "4. Training a multinomial naive Bayes model\n\nNow that we have the data in vectorized form, we can train the first model. Investigate using the Multinomial Naive Bayes model with both the CountVectorizer and TfidfVectorizer data. Which do will perform better? How come?\n\n\nTo assess the accuracies, we will print the test sets accuracy scores for both models.\n\n\n# Create a MulitnomialNB model\ntfidf_nb = MultinomialNB().fit(tfidf_train, y_train)\n\n# ... Train your model here ...\n\n# Run predict on your TF-IDF test data to get your predictions\ntfidf_nb_pred = tfidf_nb.predict(tfidf_test)\n\n# Calculate the accuracy of your predictions\ntfidf_nb_score = metrics.accuracy_score(y_test, tfidf_nb_pred)\n\n# Create a MulitnomialNB model\ncount_nb = MultinomialNB().fit(count_train, y_train)\n# ... Train your model here ...\n\n# Run predict on your count test data to get your predictions\ncount_nb_pred = count_nb.predict(count_test)\n\n# Calculate the accuracy of your predictions\ncount_nb_score = metrics.accuracy_score(y_test, count_nb_pred)\n\nprint('NaiveBayes Tfidf Score: ', tfidf_nb_score)\nprint('NaiveBayes Count Score: ', count_nb_score)\n\nNaiveBayes Tfidf Score:  0.803030303030303\nNaiveBayes Count Score:  0.7954545454545454"
  },
  {
    "objectID": "ttt/trump_trudeau_tweets.html#evaluating-our-model-using-a-confusion-matrix",
    "href": "ttt/trump_trudeau_tweets.html#evaluating-our-model-using-a-confusion-matrix",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "5. Evaluating our model using a confusion matrix",
    "text": "5. Evaluating our model using a confusion matrix\n\nWe see that the TF-IDF model performs better than the count-based approach. Based on what we know from the NLP fundamentals course, why might that be? We know that TF-IDF allows unique tokens to have a greater weight - perhaps tweeters are using specific important words that identify them! Let’s continue the investigation.\n\n\nFor classification tasks, an accuracy score doesn’t tell the whole picture. A better evaluation can be made if we look at the confusion matrix, which shows the number correct and incorrect classifications based on each class. We can use the metrics, True Positives, False Positives, False Negatives, and True Negatives, to determine how well the model performed on a given class. How many times was Trump misclassified as Trudeau?\n\n\n%matplotlib inline\n\nfrom datasets.helper_functions import plot_confusion_matrix\n\n# Calculate the confusion matrices for the tfidf_nb model and count_nb models\ntfidf_nb_cm = metrics.confusion_matrix(y_test, tfidf_nb_pred)\ncount_nb_cm = metrics.confusion_matrix(y_test, count_nb_pred)\n\n# Plot the tfidf_nb_cm confusion matrix\nplot_confusion_matrix(tfidf_nb_cm, classes=['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"TF-IDF NB Confusion Matrix\")\n\n# Plot the count_nb_cm confusion matrix without overwriting the first plot \nplot_confusion_matrix(count_nb_cm, classes=['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"Count NB Confusion matrix\", figure=1)\n\nConfusion matrix, without normalization\nConfusion matrix, without normalization"
  },
  {
    "objectID": "ttt/trump_trudeau_tweets.html#trying-out-another-classifier-linear-svc",
    "href": "ttt/trump_trudeau_tweets.html#trying-out-another-classifier-linear-svc",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "6. Trying out another classifier: Linear SVC",
    "text": "6. Trying out another classifier: Linear SVC\n\nSo the Bayesian model only has one prediction difference between the TF-IDF and count vectorizers – fairly impressive! Interestingly, there is some confusion when the predicted label is Trump but the actual tweeter is Trudeau. If we were going to use this model, we would want to investigate what tokens are causing the confusion in order to improve the model.\n\n\nNow that we’ve seen what the Bayesian model can do, how about trying a different approach? LinearSVC is another popular choice for text classification. Let’s see if using it with the TF-IDF vectors improves the accuracy of the classifier!\n\n\n# Create a LinearSVM model\ntfidf_svc = LinearSVC().fit(tfidf_train, y_train)\n\n# Run predict on your tfidf test data to get your predictions\ntfidf_svc_pred = tfidf_svc.predict(tfidf_test)\n\n# Calculate your accuracy using the metrics module\ntfidf_svc_score = metrics.accuracy_score(y_test, tfidf_svc_pred)\n\nprint(\"LinearSVC Score:   %0.3f\" % tfidf_svc_score)\n\n# Calculate the confusion matrices for the tfidf_svc model\nsvc_cm = metrics.confusion_matrix(y_test, tfidf_svc_pred)\n\n# Plot the confusion matrix using the plot_confusion_matrix function\nplot_confusion_matrix(svc_cm, classes = ['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"TF-IDF LinearSVC Confusion Matrix\")\n\nLinearSVC Score:   0.841\nConfusion matrix, without normalization"
  },
  {
    "objectID": "ttt/trump_trudeau_tweets.html#introspecting-our-top-model",
    "href": "ttt/trump_trudeau_tweets.html#introspecting-our-top-model",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "7. Introspecting our top model",
    "text": "7. Introspecting our top model\n\nWow, the LinearSVC model is even better than the Multinomial Bayesian one. Nice work! Via the confusion matrix we can see that, although there is still some confusion where Trudeau’s tweets are classified as Trump’s, the False Positive rate is better than the previous model. So, we have a performant model, right?\n\n\nWe might be able to continue tweaking and improving all of the previous models by learning more about parameter optimization or applying some better preprocessing of the tweets.\n\n\nNow let’s see what the model has learned. Using the LinearSVC Classifier with two classes (Trump and Trudeau) we can sort the features (tokens), by their weight and see the most important tokens for both Trump and Trudeau. What are the most Trump-like or Trudeau-like words? Did the model learn something useful to distinguish between these two men?\n\n\nfrom datasets.helper_functions import plot_and_return_top_features\n\n# Import pprint from pprint\nfrom pprint import pprint\n\n# Get the top features using the plot_and_return_top_features function and your top model and tfidf vectorizer\ntop_features = plot_and_return_top_features(tfidf_svc, tfidf_vectorizer)\n\n# pprint the top features\npprint(top_features)\n\n\n\n\n\n\n\n\n[(-0.3959834966911922, 'great'),\n (-0.24645580091925237, 'thank'),\n (0.06257998949180026, 'president'),\n (0.48211745246750215, 'https'),\n (0.5960555762649068, 'vietnam'),\n (0.6155609686456073, 'amp'),\n (0.7725857577713344, 'le'),\n (0.8213735137856691, 'les'),\n (0.8286549508433744, 'today'),\n (1.1869092357816051, 'du'),\n (1.3143518952322126, 'pour'),\n (1.4122560793508427, 'nous'),\n (1.4612710235935042, 'rt'),\n (1.4991808273544363, 'et'),\n (1.50564270245237, 'la'),\n (1.6567934485943738, 'canada')]"
  },
  {
    "objectID": "ttt/trump_trudeau_tweets.html#bonus-can-you-write-a-trump-or-trudeau-tweet",
    "href": "ttt/trump_trudeau_tweets.html#bonus-can-you-write-a-trump-or-trudeau-tweet",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "8. Bonus: can you write a Trump or Trudeau tweet?",
    "text": "8. Bonus: can you write a Trump or Trudeau tweet?\n\nSo, what did our model learn? It seems like it learned that Trudeau tweets in French!\n\n\nI challenge you to write your own tweet using the knowledge gained to trick the model! Use the printed list or plot above to make some inferences about what words will classify your text as Trump or Trudeau. Can you fool the model into thinking you are Trump or Trudeau?\n\n\nIf you can write French, feel free to make your Trudeau-impersonation tweet in French! As you may have noticed, these French words are common words, or, “stop words”. You could remove both English and French stop words from the tweets as a preprocessing step, but that might decrease the accuracy of the model because Trudeau is the only French-speaker in the group. If you had a dataset with more than one French speaker, this would be a useful preprocessing step.\n\n\nFuture work on this dataset could involve:\n\n\n\nAdd extra preprocessing (such as removing URLs or French stop words) and see the effects\n\n\nUse GridSearchCV to improve both your Bayesian and LinearSVC models by finding the optimal parameters\n\n\nIntrospect your Bayesian model to determine what words are more Trump- or Trudeau- like\n\n\nAdd more recent tweets to your dataset using tweepy and retrain\n\n\n\nGood luck writing your impersonation tweets – feel free to share them on Twitter!\n\n\n# Write two tweets as strings, one which you want to classify as Trump and one as Trudeau\ntrump_tweet = \"Covfeve\"\ntrudeau_tweet = \"Make Canada great again!\"\n\n# Vectorize each tweet using the TF-IDF vectorizer's transform method\n# Note: `transform` needs the string in a list object (i.e. [trump_tweet])\ntrump_tweet_vectorized = tfidf_vectorizer.transform([trump_tweet])\ntrudeau_tweet_vectorized = tfidf_vectorizer.transform([trudeau_tweet])\n\n# Call the predict method on your vectorized tweets\ntrump_tweet_pred = tfidf_svc.predict(trump_tweet_vectorized)\ntrudeau_tweet_pred = tfidf_svc.predict(trudeau_tweet_vectorized)\n\nprint(\"Predicted Trump tweet\", trump_tweet_pred)\nprint(\"Predicted Trudeau tweet\", trudeau_tweet_pred)\n\nPredicted Trump tweet ['Donald J. Trump']\nPredicted Trudeau tweet ['Justin Trudeau']"
  },
  {
    "objectID": "ttt/trump_trudeau_tweets.html#introduction-and-imports",
    "href": "ttt/trump_trudeau_tweets.html#introduction-and-imports",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "",
    "text": "So you think you can classify text? How about tweets? In this notebook, we’ll take a dive into the world of social media text classification by investigating how to properly classify tweets from two prominent North American politicians: Donald Trump and Justin Trudeau.\n\n\n\n\n\nPhoto Credit: Executive Office of the President of the United States\n\n\nTweets pose specific problems to NLP, including the fact they are shorter texts. There are also plenty of platform-specific conventions to give you hassles: mentions, #hashtags, emoji, links and short-hand phrases (ikr?). Can we overcome those challenges and build a useful classifier for these two tweeters? Yes! Let’s get started.\n\n\nTo begin, we will import all the tools we need from scikit-learn. We will need to properly vectorize our data (CountVectorizer and TfidfVectorizer). And we will also want to import some models, including MultinomialNB from the naive_bayes module, LinearSVC from the svm module and PassiveAggressiveClassifier from the linear_model module. Finally, we’ll need sklearn.metrics and train_test_split and GridSearchCV from the model_selection module to evaluate and optimize our model.\n\n\n# Set seed for reproducibility\nimport random; random.seed(53)\n\n# Import all we need from sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "heiner.atze@gmx.net, +33 7 80 84 91 20, Gentilly",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv.html#currently",
    "href": "cv.html#currently",
    "title": "Isaac Newton",
    "section": "Currently",
    "text": "Currently\nStanding on the shoulders of giants\n\nSpecialized in\nLaws of motion, gravitation, minting coins, disliking Robert Hooke\n\n\nResearch interests\nCooling, power series, optics, alchemy, planetary motions, apples.",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "EDUCATION",
    "text": "EDUCATION\nPostgraduate diploma – Biostatistics and Methods in Public Health  Paris Saclay University, Paris, France Oct 2023 — Sep 2024\nMaster level courses in :\n\nProbability and Statistics\nClinical Research\nQuantitative Epidemiology\n\nDoctor of Philosophy – Biochemistry, Microbiology  Sorbonne University, Paris, France Nov 2018 — Sep 2021\nMaster of Science - Medicinal Chemistry  Friedrich-Schiller-University, Jena, Germany Sep 2014 — Sep 2016\n\nGerman Diplom\n\nState examination - Pharmacy  * Friedrich-Schiller-University, Jena, Germany Sep 2010 — Sep 2014",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv.html#awards",
    "href": "cv.html#awards",
    "title": "Isaac Newton",
    "section": "Awards",
    "text": "Awards\n2012 President, Royal Society, London, UK\nAssociate, French Academy of Science, Paris, France",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "PUBLICATIONS",
    "text": "PUBLICATIONS\n\nbibtex_refs\nAtze et al. (2022) Bouchet et al. (2021) Bouchet et al. (2020) Le Run et al. (2020) Triboulet et al. (2019) Garscha et al. (2017)\n\n\nJournal articles\n\n\nAtze, Heiner, Yucheng Liang, Jean-Emmanuel Hugonnet, Arnaud Gutierrez, Filippo Rusconi, and Michel Arthur. 2022. “Heavy Isotope Labeling and Mass Spectrometry Reveal Unexpected Remodeling of Bacterial Cell Wall Expansion in Response to Drugs.” Elife 11: e72863.\n\n\nBouchet, Flavie, Heiner Atze, Michel Arthur, Mélanie Ethève-Quelquejeu, and Laura Iannazzo. 2021. “Traceless Staudinger Ligation to Introduce Chemical Diversity on \\(\\beta\\)-Lactamase Inhibitors of Second Generation.” Organic Letters 23 (20): 7755–58.\n\n\nBouchet, Flavie, Heiner Atze, Matthieu Fonvielle, Zainab Edoo, Michel Arthur, Mélanie Ethève-Quelquejeu, and Laura Iannazzo. 2020. “Diazabicyclooctane Functionalization for Inhibition of \\(\\beta\\)-Lactamases from Enterobacteria.” Journal of Medicinal Chemistry 63 (10): 5257–73.\n\n\nGarscha, Ulrike, Erik Romp, Simona Pace, Antonietta Rossi, Veronika Temml, Daniela Schuster, Stefanie König, et al. 2017. “Pharmacological Profile and Efficiency in Vivo of Diflapolin, the First Dual Inhibitor of 5-Lipoxygenase-Activating Protein and Soluble Epoxide Hydrolase.” Scientific Reports 7 (1): 9398.\n\n\nLe Run, Eva, Heiner Atze, Michel Arthur, and Jean-Luc Mainardi. 2020. “Impact of Relebactam-Mediated Inhibition of Mycobacterium Abscessus BlaMab \\(\\beta\\)-Lactamase on the in Vitro and Intracellular Efficacy of Imipenem.” Journal of Antimicrobial Chemotherapy 75 (2): 379–83.\n\n\nTriboulet, Sebastien, Zainab Edoo, Fabrice Compain, Clément Ourghanlian, Adrian Dupuis, Vincent Dubée, Laetitia Sutterlin, et al. 2019. “Tryptophan Fluorescence Quenching in \\(\\beta\\)-Lactam-Interacting Proteins Is Modulated by the Structure of Intermediates and Final Products of the Acylation Reaction.” ACS Infectious Diseases 5 (7): 1169–76.",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv.html#occupation",
    "href": "cv.html#occupation",
    "title": "Isaac Newton",
    "section": "Occupation",
    "text": "Occupation\n1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "Curriculum Vitae",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE\n\nPharmacist\nPharmacie Attal  Fontenay-aux-Roses, France Oct 2021 – present\nPharmaceutical counseling of patients\nResponsible for communication with medical staff in nursing homes\nSupervision of technical staff and pharmacy students\nOther Pharmacies  Jena, Germany Nov 2015 – May 2018\n\n\nPre-registration pharmacist\nF. Hoffmann-La Roche  Basel, Switzerland May 2015 – Oct 2015",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv.html#presentations-and-workshops",
    "href": "cv.html#presentations-and-workshops",
    "title": "Curriculum Vitae",
    "section": "PRESENTATIONS AND WORKSHOPS",
    "text": "PRESENTATIONS AND WORKSHOPS\n\nTalk: Misadventures with Reproducibility in R (30 Nov 2022, R Ladies Melbourne Meetup)\nTalk: Designing R Packages (4 Oct 2022, Monash EBS Data Science Research Software Study Group)\nTalk: Quarto Websites as Research Compendiums (16 Aug 2022, Monash EBS Data Science Research Software Study Group)\nWorkshop: Writing academic papers with Rmarkdown and friends (9 Aug 2022, Monash University)",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv.html#references",
    "href": "cv.html#references",
    "title": "Curriculum Vitae",
    "section": "References",
    "text": "References\nAvailable upon request",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv.html#research-experience",
    "href": "cv.html#research-experience",
    "title": "Curriculum Vitae",
    "section": "RESEARCH EXPERIENCE",
    "text": "RESEARCH EXPERIENCE\nDoctoral Researcher – Biochemistry, Microbiology  INSERM, Paris, France Jun 2018 — Sep 2021\nResearcher in Team 12 “Structures implicated in antibiotic resistance” at the Centre de Recherche des Cordeliers, Paris\nTwo main axes of research:\n\nBiological characterization of new generation β-lactamase inhibitors\nin vitro and in vivo characterization of inhibitors, data analysis and interpretation, feedback into the consult-design-test-repeat cycle in collaboration with the team of organic chemists\nFundamental research on cell wall metabolism in gram-negative bacteria\nDe novo method development: isotopic labeling of cell cultures, sample preparation, analysis by mass spectrometry, custom data analysis tools and pipelines\n\nKey achievements and skills:\n\nExploration of the chemical space around the core inhibitor and identification of curcial ligand-target-interactions\nHandling and managing a large amount of results and data from biological experiments\n\nResearch assistant – Medicinal and Pharmaceutical Chemistry  Friedrich-Schiller-University, Jena, Germany Sep 2014 — Sep 2016\nBiological charaterization of putative anti-inflammatory substances, fundamental research on signaling cascades in inflammation using biochemical methods and imaging techniques",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "SKILLS",
    "text": "SKILLS\n\nLanguages\nGerman: Native Speaker\nEnglish: Professional proficiency\nFrench: Professional proficiency\n\n\nTechnical\n\nProgramming languages:\n\n\n : advanced",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv.html#doctoral-researcher-biochemistry-microbiology",
    "href": "cv.html#doctoral-researcher-biochemistry-microbiology",
    "title": "Curriculum Vitae",
    "section": "Doctoral Researcher – Biochemistry, Microbiology",
    "text": "Doctoral Researcher – Biochemistry, Microbiology\nINSERM, Paris, France Jun 2018 — Sep 2021\nResearcher in Team 12 “Structures implicated in antibiotic resistance” at the Centre de Recherche des Cordeliers, Paris\nTwo main axes of research:\n\nCharacterization of new generation \\(\\beta\\)-lactamase inhibitors",
    "crumbs": [
      "CV"
    ]
  }
]