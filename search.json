[
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "heiner.atze@gmx.net, +33 7 80 84 91 20, Gentilly",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#education",
    "href": "cv/cv.html#education",
    "title": "Curriculum Vitae",
    "section": "EDUCATION",
    "text": "EDUCATION\nPostgraduate diploma – Biostatistics and Methods in Public Health  Paris Saclay University, Paris, France Oct 2023 — Sep 2024\nMaster level courses in :\n\nProbability and Statistics\nClinical Research\nQuantitative Epidemiology\n\nDoctor of Philosophy – Biochemistry, Microbiology  Sorbonne University, Paris, France Nov 2018 — Sep 2021\nMaster of Science - Medicinal Chemistry  Friedrich-Schiller-University, Jena, Germany Sep 2014 — Sep 2016\n\nGerman Diplom\n\nState examination - Pharmacy  * Friedrich-Schiller-University, Jena, Germany Sep 2010 — Sep 2014",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#research-experience",
    "href": "cv/cv.html#research-experience",
    "title": "Curriculum Vitae",
    "section": "RESEARCH EXPERIENCE",
    "text": "RESEARCH EXPERIENCE\nDoctoral Researcher – Biochemistry, Microbiology  INSERM, Paris, France Jun 2018 — Sep 2021\nResearcher in Team 12 “Structures implicated in antibiotic resistance” at the Centre de Recherche des Cordeliers, Paris\nTwo main axes of research:\n\nBiological characterization of new generation β-lactamase inhibitors\nin vitro and in vivo characterization of inhibitors, data analysis and interpretation, feedback into the consult-design-test-repeat cycle in collaboration with the team of organic chemists\nFundamental research on cell wall metabolism in gram-negative bacteria\nDe novo method development: isotopic labeling of cell cultures, sample preparation, analysis by mass spectrometry, custom data analysis tools and pipelines\n\nKey achievements and skills:\n\nExploration of the chemical space around the core inhibitor and identification of curcial ligand-target-interactions\nHandling and managing a large amount of results and data from biological experiments\n\nResearch assistant – Medicinal and Pharmaceutical Chemistry  Friedrich-Schiller-University, Jena, Germany Sep 2014 — Sep 2016\nBiological charaterization of putative anti-inflammatory substances, fundamental research on signaling cascades in inflammation using biochemical methods and imaging techniques",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#work-experience",
    "href": "cv/cv.html#work-experience",
    "title": "Curriculum Vitae",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE\n\nPharmacist\nPharmacie Attal  Fontenay-aux-Roses, France Oct 2021 – present\nPharmaceutical counseling of patients\nResponsible for communication with medical staff in nursing homes\nSupervision of technical staff and pharmacy students\nOther Pharmacies  Jena, Germany Nov 2015 – May 2018\n\n\nPre-registration pharmacist\nF. Hoffmann-La Roche  Basel, Switzerland May 2015 – Oct 2015",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#skills",
    "href": "cv/cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "SKILLS",
    "text": "SKILLS\n\nLanguages\nGerman: Native Speaker\nEnglish: Professional proficiency\nFrench: Professional proficiency\n\n\nTechnical\n\nProgramming languages:\n\n\n : advanced",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#publications",
    "href": "cv/cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "PUBLICATIONS",
    "text": "PUBLICATIONS\n\nbibtex_refs\nAtze et al. (2022) Bouchet et al. (2021) Bouchet et al. (2020) Le Run et al. (2020) Triboulet et al. (2019) Garscha et al. (2017)\n\n\nJournal articles\n\n\nAtze, Heiner, Yucheng Liang, Jean-Emmanuel Hugonnet, Arnaud Gutierrez, Filippo Rusconi, and Michel Arthur. 2022. “Heavy Isotope Labeling and Mass Spectrometry Reveal Unexpected Remodeling of Bacterial Cell Wall Expansion in Response to Drugs.” Elife 11: e72863.\n\n\nBouchet, Flavie, Heiner Atze, Michel Arthur, Mélanie Ethève-Quelquejeu, and Laura Iannazzo. 2021. “Traceless Staudinger Ligation to Introduce Chemical Diversity on \\(\\beta\\)-Lactamase Inhibitors of Second Generation.” Organic Letters 23 (20): 7755–58.\n\n\nBouchet, Flavie, Heiner Atze, Matthieu Fonvielle, Zainab Edoo, Michel Arthur, Mélanie Ethève-Quelquejeu, and Laura Iannazzo. 2020. “Diazabicyclooctane Functionalization for Inhibition of \\(\\beta\\)-Lactamases from Enterobacteria.” Journal of Medicinal Chemistry 63 (10): 5257–73.\n\n\nGarscha, Ulrike, Erik Romp, Simona Pace, Antonietta Rossi, Veronika Temml, Daniela Schuster, Stefanie König, et al. 2017. “Pharmacological Profile and Efficiency in Vivo of Diflapolin, the First Dual Inhibitor of 5-Lipoxygenase-Activating Protein and Soluble Epoxide Hydrolase.” Scientific Reports 7 (1): 9398.\n\n\nLe Run, Eva, Heiner Atze, Michel Arthur, and Jean-Luc Mainardi. 2020. “Impact of Relebactam-Mediated Inhibition of Mycobacterium Abscessus BlaMab \\(\\beta\\)-Lactamase on the in Vitro and Intracellular Efficacy of Imipenem.” Journal of Antimicrobial Chemotherapy 75 (2): 379–83.\n\n\nTriboulet, Sebastien, Zainab Edoo, Fabrice Compain, Clément Ourghanlian, Adrian Dupuis, Vincent Dubée, Laetitia Sutterlin, et al. 2019. “Tryptophan Fluorescence Quenching in \\(\\beta\\)-Lactam-Interacting Proteins Is Modulated by the Structure of Intermediates and Final Products of the Acylation Reaction.” ACS Infectious Diseases 5 (7): 1169–76.",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "cv/cv.html#references",
    "href": "cv/cv.html#references",
    "title": "Curriculum Vitae",
    "section": "References",
    "text": "References\nAvailable upon request",
    "crumbs": [
      "CV"
    ]
  },
  {
    "objectID": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html",
    "href": "datacamp_projects/guided/python/nlp_movies_similarity/notebook.html",
    "title": "1. Import and observe dataset",
    "section": "",
    "text": "We all love watching movies! There are some movies we like, some we don’t. Most people have a preference for movies of a similar genre. Some of us love watching action movies, while some of us like watching horror. Some of us like watching movies that have ninjas in them, while some of us like watching superheroes.\n\n\nMovies within a genre often share common base parameters. Consider the following two movies:\n\n\n \n\n\nBoth movies, 2001: A Space Odyssey and Close Encounters of the Third Kind, are movies based on aliens coming to Earth. I’ve seen both, and they indeed share many similarities. We could conclude that both of these fall into the same genre of movies based on intuition, but that’s no fun in a data science context. In this notebook, we will quantify the similarity of movies based on their plot summaries available on IMDb and Wikipedia, then separate them into groups, also known as clusters. We’ll create a dendrogram to represent how closely the movies are related to each other.\n\n\nLet’s start by importing the dataset and observing the data provided.\n\n\n# Import modulesimport numpy as npimport pandas as pdimport nltk# Set seed for reproducibilitynp.random.seed(5)# Read in IMDb and Wikipedia movie data (both in same file)movies_df = pd.read_csv('datasets/movies.csv')print(\"Number of movies loaded: %s \" % (len(movies_df)))# Display the datamovies_df\n\nNumber of movies loaded: 100 \n\n\n\n\n\n\n\n\n\nrank\ntitle\ngenre\nwiki_plot\nimdb_plot\n\n\n\n\n0\n0\nThe Godfather\n[u' Crime', u' Drama']\nOn the day of his only daughter's wedding, Vit...\nIn late summer 1945, guests are gathered for t...\n\n\n1\n1\nThe Shawshank Redemption\n[u' Crime', u' Drama']\nIn 1947, banker Andy Dufresne is convicted of ...\nIn 1947, Andy Dufresne (Tim Robbins), a banker...\n\n\n2\n2\nSchindler's List\n[u' Biography', u' Drama', u' History']\nIn 1939, the Germans move Polish Jews into the...\nThe relocation of Polish Jews from surrounding...\n\n\n3\n3\nRaging Bull\n[u' Biography', u' Drama', u' Sport']\nIn a brief scene in 1964, an aging, overweight...\nThe film opens in 1964, where an older and fat...\n\n\n4\n4\nCasablanca\n[u' Drama', u' Romance', u' War']\nIt is early December 1941. American expatriate...\nIn the early years of World War II, December 1...\n\n\n5\n5\nOne Flew Over the Cuckoo's Nest\n[u' Drama']\nIn 1963 Oregon, Randle Patrick \"Mac\" McMurphy ...\nIn 1963 Oregon, Randle Patrick McMurphy (Nicho...\n\n\n6\n6\nGone with the Wind\n[u' Drama', u' Romance', u' War']\n\\nPart 1\\n \\n Part 1 Part 1 \\n \\n On the...\nThe film opens in Tara, a cotton plantation ow...\n\n\n7\n7\nCitizen Kane\n[u' Drama', u' Mystery']\n\\n\\n\\n\\nOrson Welles as Charles Foster Kane\\n\\...\nIt's 1941, and newspaper tycoon Charles Foster...\n\n\n8\n8\nThe Wizard of Oz\n[u' Adventure', u' Family', u' Fantasy', u' Mu...\nThe film starts in sepia-tinted Kansas in the ...\nDorothy Gale (Judy Garland) is an orphaned tee...\n\n\n9\n9\nTitanic\n[u' Drama', u' Romance']\nIn 1996, treasure hunter Brock Lovett and his ...\nIn 1996, treasure hunter Brock Lovett and his ...\n\n\n10\n10\nLawrence of Arabia\n[u' Adventure', u' Biography', u' Drama', u' H...\n] \\n The film is presented in two parts, s...\nIn 1935, T. E. Lawrence (Peter O'Toole) is kil...\n\n\n11\n11\nThe Godfather: Part II\n[u' Crime', u' Drama']\n\\nIn 1901 Corleone, Sicily, nine-year-old Vito...\nThe Godfather Part II presents two parallel st...\n\n\n12\n12\nPsycho\n[u' Horror', u' Mystery', u' Thriller']\nPatrick Bateman is a wealthy investment banker...\nIn a Phoenix hotel room on a Friday afternoon,...\n\n\n13\n13\nSunset Blvd.\n[u' Drama', u' Film-Noir']\nAt a Sunset Boulevard mansion, the body of Joe...\nThe film opens with the camera tracking down S...\n\n\n14\n14\nVertigo\n[u' Mystery', u' Romance', u' Thriller']\nridge, Fort Point\\n\\n \\n \\n\\n\\n\"Madeleine\" a...\nA woman's face gives way to a kaleidoscope of ...\n\n\n15\n15\nOn the Waterfront\n[u' Crime', u' Drama']\nMob-connected union boss Johnny Friendly (Lee ...\nTerry Malloy (Marlon Brando) once dreamt of be...\n\n\n16\n16\nForrest Gump\n[u' Drama', u' Romance']\nWhile waiting at a bus stop in 1981, Forrest G...\nThe film begins with a feather falling to the ...\n\n\n17\n17\nThe Sound of Music\n[u' Biography', u' Drama', u' Family', u' Musi...\nIn 1938, while living as a young postulant at ...\nThe widowed, retired Austrian naval officer, C...\n\n\n18\n18\nWest Side Story\n[u' Crime', u' Drama', u' Musical', u' Romance...\n] ] \\n In the West Side's Lincoln Square ne...\nA fight set to music between an American gang,...\n\n\n19\n19\nStar Wars\n[u' Action', u' Adventure', u' Fantasy', u' Sc...\nThe galaxy is in a civil war, and spies for th...\n\\nNote: Italicized paragraphs denote scenes ad...\n\n\n20\n20\nE.T. the Extra-Terrestrial\n[u' Adventure', u' Family', u' Sci-Fi']\nIn a California forest, a group of alien botan...\nIn a forested area overlooking a sprawling sub...\n\n\n21\n21\n2001: A Space Odyssey\n[u' Mystery', u' Sci-Fi']\nThe film consists of four major sections, all ...\nTo Richard Strauss' tone poem \"Thus Spake Zara...\n\n\n22\n22\nThe Silence of the Lambs\n[u' Crime', u' Drama', u' Thriller']\nis pulled from her training at the FBI Academy...\nPromising FBI Academy student Clarice Starling...\n\n\n23\n23\nChinatown\n[u' Drama', u' Mystery', u' Thriller']\nA woman identifying herself as Evelyn Mulwray ...\nSet in 1937 Los Angeles, a private investigato...\n\n\n24\n24\nThe Bridge on the River Kwai\n[u' Adventure', u' Drama', u' War']\nIn World War II, British prisoners arrive at a...\nthis synopsis is primarily from the wikipedia ...\n\n\n25\n25\nSingin' in the Rain\n[u' Comedy', u' Musical', u' Romance']\nDon Lockwood is a popular silent film star wit...\nDon Lockwood (Gene Kelly) is a popular silent ...\n\n\n26\n26\nIt's a Wonderful Life\n[u' Drama', u' Family', u' Fantasy']\n\\n\\n\\n\\nDonna Reed (as Mary Bailey) and James ...\nThis movie is about a divine intervention by a...\n\n\n27\n27\nSome Like It Hot\n[u' Comedy']\nIt is February 1929 in the city of Chicago. Jo...\nJoe and Jerry, a saxophonist and bassist, resp...\n\n\n28\n28\n12 Angry Men\n[u' Drama']\nThe story begins in a New York City courthous...\nA teenaged Hispanic boy has just been tried fo...\n\n\n29\n29\nDr. Strangelove or: How I Learned to Stop Worr...\n[u' Comedy', u' War']\nUnited States Air Force Brigadier General Jack...\nAt the Burpelson U.S. Air Force Base somewhere...\n\n\n...\n...\n...\n...\n...\n...\n\n\n70\n70\nRain Man\n[u' Drama']\nCharlie Babbitt is in the middle of importing ...\nCharlie Babbitt (Tom Cruise), a Los Angeles ca...\n\n\n71\n71\nAnnie Hall\n[u' Comedy', u' Drama', u' Romance']\nThe comedian Alvy Singer (Woody Allen) is tryi...\nAnnie Hall is a film about a comedian, Alvy Si...\n\n\n72\n72\nOut of Africa\n[u' Biography', u' Drama', u' Romance']\nThe story begins in 1913 in Denmark, when Kare...\n[Out Of Africa]A well-heeled Danish lady goes ...\n\n\n73\n73\nGood Will Hunting\n[u' Drama']\nTwenty-year-old Will Hunting (Damon) of South ...\nThough Will Hunting (Matt Damon) has genius-le...\n\n\n74\n74\nTerms of Endearment\n[u' Comedy', u' Drama']\nAurora Greenway (Shirley MacLaine) and her dau...\nNaN\n\n\n75\n75\nTootsie\n[u' Comedy', u' Drama', u' Romance']\nMichael Dorsey (Dustin Hoffman) is a respected...\nMichael Dorsey is an actor living and working ...\n\n\n76\n76\nFargo\n[u' Crime', u' Drama', u' Thriller']\nIn the winter of 1987, Minneapolis car salesma...\nThe movie opens with a car towing a new tan Ol...\n\n\n77\n77\nGiant\n[u' Drama', u' Romance']\nJordan \"Bick\" Benedict (Rock Hudson), head of ...\nIn the early 1920s, Jordan \"Bick\" Benedict (Ro...\n\n\n78\n78\nThe Grapes of Wrath\n[u' Drama']\nThe film opens with Tom Joad (Henry Fonda), re...\nAfter serving four years in prison for killing...\n\n\n79\n79\nShane\n[u' Drama', u' Romance', u' Western']\n\\n\\n\\n\\nAlan Ladd and Jean Arthur\\n\\n \\n \\n\\...\nNaN\n\n\n80\n80\nThe Green Mile\n[u' Crime', u' Drama', u' Fantasy', u' Mystery']\nIn a Louisiana nursing home in 1999, Paul Edge...\nThe movie begins with an old man named Paul Ed...\n\n\n81\n81\nClose Encounters of the Third Kind\n[u' Drama', u' Sci-Fi']\nIn the Sonoran Desert, French scientist Claude...\nIn what appers to be the Sonoran Desert; in or...\n\n\n82\n82\nNetwork\n[u' Drama']\nHoward Beale, the longtime anchor of the Union...\nNaN\n\n\n83\n83\nNashville\n[u' Drama', u' Music']\nThe overarching plot takes place over five day...\nThe overarching plot takes place over five day...\n\n\n84\n84\nThe Graduate\n[u' Comedy', u' Drama', u' Romance']\nBenjamin Braddock, going on from twenty to twe...\nThe film explores the life of 21-year-old Ben ...\n\n\n85\n85\nAmerican Graffiti\n[u' Comedy', u' Drama']\nIn late August 1962 recent high school graduat...\nIt's the last night of the summer in 1962, and...\n\n\n86\n86\nPulp Fiction\n[u' Crime', u' Drama', u' Thriller']\nThe Diner\" \"Prologue—The Diner\" \\n \"Pumpkin...\nLate one morning in the Hawthorne Grill, a res...\n\n\n87\n87\nThe African Queen\n[u' Adventure', u' Romance', u' War']\nRobert Morley and Katharine Hepburn play Samue...\nAn English spinster, Rose (Katharine Hepburn),...\n\n\n88\n88\nStagecoach\n[u' Adventure', u' Western']\nIn 1880, a motley group of strangers boards th...\nNaN\n\n\n89\n89\nMutiny on the Bounty\n[u' Adventure', u' Drama', u' History']\nIn the year 1787, the Bounty sets sail from En...\nNaN\n\n\n90\n90\nThe Maltese Falcon\n[u' Drama', u' Film-Noir', u' Mystery']\n\\n\\n\\nIn 1539 the Knight Templars of Malta, pa...\nPrivate eye Sam Spade and his partner Miles Ar...\n\n\n91\n91\nA Clockwork Orange\n[u' Crime', u' Drama', u' Sci-Fi']\nIn futuristic London, Alex DeLarge is the lead...\nA bit of the old ultra-violence\".London, Engla...\n\n\n92\n92\nTaxi Driver\n[u' Crime', u' Drama']\nTravis Bickle, an honorably discharged U.S. Ma...\nTravis Bickle (Robert De Niro) goes to a New Y...\n\n\n93\n93\nWuthering Heights\n[u' Drama', u' Romance']\nA traveller named Lockwood (Miles Mander) is c...\nNaN\n\n\n94\n94\nDouble Indemnity\n[u' Crime', u' Drama', u' Film-Noir', u' Thril...\n\\n\\n\\n\\nNeff confesses into a Dictaphone.\\n\\n ...\nWalter Neff (MacMurray) is a successful insura...\n\n\n95\n95\nRebel Without a Cause\n[u' Drama']\n\\n\\n\\n\\nJim Stark is in police custody.\\n\\n \\...\nShortly after moving to Los Angeles with his p...\n\n\n96\n96\nRear Window\n[u' Mystery', u' Thriller']\n\\n\\n\\n\\nJames Stewart as L.B. Jefferies\\n\\n \\...\nL.B. \"Jeff\" Jeffries (James Stewart) recuperat...\n\n\n97\n97\nThe Third Man\n[u' Film-Noir', u' Mystery', u' Thriller']\n\\n\\n\\n\\nSocial network mapping all major chara...\nSights of Vienna, Austria, flash across the sc...\n\n\n98\n98\nNorth by Northwest\n[u' Mystery', u' Thriller']\nAdvertising executive Roger O. Thornhill is mi...\nAt the end of an ordinary work day, advertisin...\n\n\n99\n99\nYankee Doodle Dandy\n[u' Biography', u' Drama', u' Musical']\n\\n In the early days of World War II, Cohan ...\nNaN\n\n\n\n\n100 rows × 5 columns\n\n\n\n## 2. Combine Wikipedia and IMDb plot summaries\n\nThe dataset we imported currently contains two columns titled wiki_plot and imdb_plot. They are the plot found for the movies on Wikipedia and IMDb, respectively. The text in the two columns is similar, however, they are often written in different tones and thus provide context on a movie in a different manner of linguistic expression. Further, sometimes the text in one column may mention a feature of the plot that is not present in the other column. For example, consider the following plot extracts from The Godfather:\n\n\n\nWikipedia: “On the day of his only daughter’s wedding, Vito Corleone”\n\n\nIMDb: “In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleone’s daughter Connie”\n\n\n\nWhile the Wikipedia plot only mentions it is the day of the daughter’s wedding, the IMDb plot also mentions the year of the scene and the name of the daughter.\n\n\nLet’s combine both the columns to avoid the overheads in computation associated with extra columns to process.\n\n\n# Combine wiki_plot and imdb_plot into a single columnmovies_df['plot'] = movies_df['wiki_plot'].astype(str) + \"\\n\" + \\                 movies_df['imdb_plot'].astype(str)# Inspect the new DataFramemovies_df.head()\n\n\n\n\n\n\n\n\nrank\ntitle\ngenre\nwiki_plot\nimdb_plot\nplot\n\n\n\n\n0\n0\nThe Godfather\n[u' Crime', u' Drama']\nOn the day of his only daughter's wedding, Vit...\nIn late summer 1945, guests are gathered for t...\nOn the day of his only daughter's wedding, Vit...\n\n\n1\n1\nThe Shawshank Redemption\n[u' Crime', u' Drama']\nIn 1947, banker Andy Dufresne is convicted of ...\nIn 1947, Andy Dufresne (Tim Robbins), a banker...\nIn 1947, banker Andy Dufresne is convicted of ...\n\n\n2\n2\nSchindler's List\n[u' Biography', u' Drama', u' History']\nIn 1939, the Germans move Polish Jews into the...\nThe relocation of Polish Jews from surrounding...\nIn 1939, the Germans move Polish Jews into the...\n\n\n3\n3\nRaging Bull\n[u' Biography', u' Drama', u' Sport']\nIn a brief scene in 1964, an aging, overweight...\nThe film opens in 1964, where an older and fat...\nIn a brief scene in 1964, an aging, overweight...\n\n\n4\n4\nCasablanca\n[u' Drama', u' Romance', u' War']\nIt is early December 1941. American expatriate...\nIn the early years of World War II, December 1...\nIt is early December 1941. American expatriate...\n\n\n\n\n\n\n\n## 3. Tokenization\n\nTokenization is the process by which we break down articles into individual sentences or words, as needed. Besides the tokenization method provided by NLTK, we might have to perform additional filtration to remove tokens which are entirely numeric values or punctuation.\n\n\nWhile a program may fail to build context from “While waiting at a bus stop in 1981” (Forrest Gump), because this string would not match in any dictionary, it is possible to build context from the words “while”, “waiting” or “bus” because they are present in the English dictionary.\n\n\nLet us perform tokenization on a small extract from The Godfather.\n\n\n# Tokenize a paragraph into sentences and store in sent_tokenizedsent_tokenized = [sent for sent in nltk.sent_tokenize(\"\"\"                        Today (May 19, 2016) is his only daughter's wedding.                         Vito Corleone is the Godfather.                        \"\"\")]# Word Tokenize first sentence from sent_tokenized, save as words_tokenizedwords_tokenized = [word for word in nltk.word_tokenize(sent_tokenized[0])]# Remove tokens that do not contain any letters from words_tokenizedimport refiltered = [word for word in words_tokenized if re.search('[a-zA-Z]', word)]# Display filtered words to observe words after tokenizationfiltered\n\n['Today', 'May', 'is', 'his', 'only', 'daughter', \"'s\", 'wedding']\n\n\n## 4. Stemming\n\nStemming is the process by which we bring down a word from its different forms to the root word. This helps us establish meaning to different forms of the same words without having to deal with each form separately. For example, the words ‘fishing’, ‘fished’, and ‘fisher’ all get stemmed to the word ‘fish’.\n\n\nConsider the following sentences:\n\n\n\n“Young William Wallace witnesses the treachery of Longshanks” ~ Gladiator\n\n\n“escapes to the city walls only to witness Cicero’s death” ~ Braveheart\n\n\n\nInstead of building separate dictionary entries for both witnesses and witness, which mean the same thing outside of quantity, stemming them reduces them to ‘wit’.\n\n\nThere are different algorithms available for stemming such as the Porter Stemmer, Snowball Stemmer, etc. We shall use the Snowball Stemmer.\n\n\n# Import the SnowballStemmer to perform stemmingfrom nltk.stem.snowball import SnowballStemmer# Create an English language SnowballStemmer objectstemmer = SnowballStemmer(\"english\")# Print filtered to observe words without stemmingprint(\"Without stemming: \", filtered)# Stem the words from filtered and store in stemmed_wordsstemmed_words = [stemmer.stem(word) for word in filtered]# Print the stemmed_words to observe words after stemmingprint(\"After stemming:   \", stemmed_words)\n\nWithout stemming:  ['Today', 'May', 'is', 'his', 'only', 'daughter', \"'s\", 'wedding']\nAfter stemming:    ['today', 'may', 'is', 'his', 'onli', 'daughter', \"'s\", 'wed']\n\n\n## 5. Club together Tokenize & Stem\n\nWe are now able to tokenize and stem sentences. But we may have to use the two functions repeatedly one after the other to handle a large amount of data, hence we can think of wrapping them in a function and passing the text to be tokenized and stemmed as the function argument. Then we can pass the new wrapping function, which shall perform both tokenizing and stemming instead of just tokenizing, as the tokenizer argument while creating the TF-IDF vector of the text.\n\n\nWhat difference does it make though? Consider the sentence from the plot of The Godfather: “Today (May 19, 2016) is his only daughter’s wedding.” If we do a ‘tokenize-only’ for this sentence, we have the following result:\n\n\n\n‘today’, ‘may’, ‘is’, ‘his’, ‘only’, ‘daughter’, “‘s”, ’wedding’\n\n\n\nBut when we do a ‘tokenize-and-stem’ operation we get:\n\n\n\n‘today’, ‘may’, ‘is’, ‘his’, ‘onli’, ‘daughter’, “‘s”, ’wed’\n\n\n\nAll the words are in their root form, which will lead to a better establishment of meaning as some of the non-root forms may not be present in the NLTK training corpus.\n\n\n# Define a function to perform both stemming and tokenizationdef tokenize_and_stem(text):        # Tokenize by sentence, then by word    tokens = [sent for sent in nltk.sent_tokenize(text)]    tokens = [word for x in tokens for word in nltk.word_tokenize(x)]        # Filter out raw tokens to remove noise    filtered_tokens = [token for token in tokens if re.search('[a-zA-Z]', token)]        # Stem the filtered_tokens    stems = [stemmer.stem(token) for token in filtered_tokens]        return stemswords_stemmed = tokenize_and_stem(\"Today (May 19, 2016) is his only daughter's wedding.\")print(words_stemmed)\n\n['today', 'may', 'is', 'his', 'onli', 'daughter', \"'s\", 'wed']\n\n\n## 6. Create TfidfVectorizer\n\nComputers do not understand text. These are machines only capable of understanding numbers and performing numerical computation. Hence, we must convert our textual plot summaries to numbers for the computer to be able to extract meaning from them. One simple method of doing this would be to count all the occurrences of each word in the entire vocabulary and return the counts in a vector. Enter CountVectorizer.\n\n\nConsider the word ‘the’. It appears quite frequently in almost all movie plots and will have a high count in each case. But obviously, it isn’t the theme of all the movies! Term Frequency-Inverse Document Frequency (TF-IDF) is one method which overcomes the shortcomings of CountVectorizer. The Term Frequency of a word is the measure of how often it appears in a document, while the Inverse Document Frequency is the parameter which reduces the importance of a word if it frequently appears in several documents.\n\n\nFor example, when we apply the TF-IDF on the first 3 sentences from the plot of The Wizard of Oz, we are told that the most important word there is ‘Toto’, the pet dog of the lead character. This is because the movie begins with ‘Toto’ biting someone due to which the journey of Oz begins!\n\n\nIn simplest terms, TF-IDF recognizes words which are unique and important to any given document. Let’s create one for our purposes.\n\n\n# Import TfidfVectorizer to create TF-IDF vectorsfrom sklearn.feature_extraction.text import TfidfVectorizer# Instantiate TfidfVectorizer object with stopwords and tokenizer# parameters for efficient processing of texttfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,                                 min_df=0.2, stop_words='english',                                 use_idf=True, tokenizer=tokenize_and_stem,                                 ngram_range=(1,3))\n\n## 7. Fit transform TfidfVectorizer\n\nOnce we create a TF-IDF Vectorizer, we must fit the text to it and then transform the text to produce the corresponding numeric form of the data which the computer will be able to understand and derive meaning from. To do this, we use the fit_transform() method of the TfidfVectorizer object.\n\n\nIf we observe the TfidfVectorizer object we created, we come across a parameter stopwords. ‘stopwords’ are those words in a given text which do not contribute considerably towards the meaning of the sentence and are generally grammatical filler words. For example, in the sentence ‘Dorothy Gale lives with her dog Toto on the farm of her Aunt Em and Uncle Henry’, we could drop the words ‘her’ and ‘the’, and still have a similar overall meaning to the sentence. Thus, ‘her’ and ‘the’ are stopwords and can be conveniently dropped from the sentence.\n\n\nOn setting the stopwords to ‘english’, we direct the vectorizer to drop all stopwords from a pre-defined list of English language stopwords present in the nltk module. Another parameter, ngram_range, defines the length of the ngrams to be formed while vectorizing the text.\n\n\n# Fit and transform the tfidf_vectorizer with the \"plot\" of each movie# to create a vector representation of the plot summariestfidf_matrix = tfidf_vectorizer.fit_transform([x for x in movies_df[\"plot\"]])print(tfidf_matrix.shape)\n\n(100, 564)\n\n\n## 8. Import KMeans and create clusters\n\nTo determine how closely one movie is related to the other by the help of unsupervised learning, we can use clustering techniques. Clustering is the method of grouping together a number of items such that they exhibit similar properties. According to the measure of similarity desired, a given sample of items can have one or more clusters.\n\n\nA good basis of clustering in our dataset could be the genre of the movies. Say we could have a cluster ‘0’ which holds movies of the ‘Drama’ genre. We would expect movies like Chinatown or Psycho to belong to this cluster. Similarly, the cluster ‘1’ in this project holds movies which belong to the ‘Adventure’ genre (Lawrence of Arabia and the Raiders of the Lost Ark, for example).\n\n\nK-means is an algorithm which helps us to implement clustering in Python. The name derives from its method of implementation: the given sample is divided into K clusters where each cluster is denoted by the mean of all the items lying in that cluster.\n\n\nWe get the following distribution for the clusters:\n\n\n\n\n\n# Import k-means to perform clustersfrom sklearn.cluster import KMeans# Create a KMeans object with 5 clusters and save as kmkm = KMeans(n_clusters=5)# Fit the k-means object with tfidf_matrixkm.fit(tfidf_matrix)clusters = km.labels_.tolist()# Create a column cluster to denote the generated cluster for each moviemovies_df[\"cluster\"] = clusters# Display number of films per cluster (clusters from 0 to 4)movies_df['cluster'].value_counts() \n\n2    35\n1    21\n3    20\n0    17\n4     7\nName: cluster, dtype: int64\n\n\n## 9. Calculate similarity distance\n\nConsider the following two sentences from the movie The Wizard of Oz:\n\n\n\n“they find in the Emerald City”\n\n\n“they finally reach the Emerald City”\n\n\n\nIf we put the above sentences in a CountVectorizer, the vocabulary produced would be “they, find, in, the, Emerald, City, finally, reach” and the vectors for each sentence would be as follows:\n\n\n\n1, 1, 1, 1, 1, 1, 0, 0\n\n\n1, 0, 0, 1, 1, 1, 1, 1\n\n\n\nWhen we calculate the cosine angle formed between the vectors represented by the above, we get a score of 0.667. This means the above sentences are very closely related. Similarity distance is 1 - cosine similarity angle. This follows from that if the vectors are similar, the cosine of their angle would be 1 and hence, the distance between then would be 1 - 1 = 0.\n\n\nLet’s calculate the similarity distance for all of our movies.\n\n\n# Import cosine_similarity to calculate similarity of movie plotsfrom sklearn.metrics.pairwise import cosine_similarity# Calculate the similarity distancesimilarity_distance = 1 - cosine_similarity(tfidf_matrix)\n\n## 10. Import Matplotlib, Linkage, and Dendrograms\n\nWe shall now create a tree-like diagram (called a dendrogram) of the movie titles to help us understand the level of similarity between them visually. Dendrograms help visualize the results of hierarchical clustering, which is an alternative to k-means clustering. Two pairs of movies at the same level of hierarchical clustering are expected to have similar strength of similarity between the corresponding pairs of movies. For example, the movie Fargo would be as similar to North By Northwest as the movie Platoon is to Saving Private Ryan, given both the pairs exhibit the same level of the hierarchy.\n\n\nLet’s import the modules we’ll need to create our dendrogram.\n\n\n# Import matplotlib.pyplot for plotting graphsimport matplotlib.pyplot as plt# Configure matplotlib to display the output inline%matplotlib inline# Import modules necessary to plot dendrogramfrom scipy.cluster.hierarchy import linkage, dendrogram\n\nDEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n\n\n## 11. Create merging and plot dendrogram\n\nWe shall plot a dendrogram of the movies whose similarity measure will be given by the similarity distance we previously calculated. The lower the similarity distance between any two movies, the lower their linkage will make an intercept on the y-axis. For instance, the lowest dendrogram linkage we shall discover will be between the movies, It’s a Wonderful Life and A Place in the Sun. This indicates that the movies are very similar to each other in their plots.\n\n\n# Create mergings matrix mergings = linkage(similarity_distance, method='complete')# Plot the dendrogram, using title as label columndendrogram_ = dendrogram(mergings,               labels=[x for x in movies_df[\"title\"]],               leaf_rotation=90,               leaf_font_size=16,)# Adjust the plotfig = plt.gcf()_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]fig.set_size_inches(108, 21)# Show the plotted dendrogramplt.show()\n\nDEBUG:matplotlib.axes._base:update_title_pos\nDEBUG:matplotlib.axes._base:update_title_pos\nDEBUG:matplotlib.axes._base:update_title_pos\nDEBUG:matplotlib.axes._base:update_title_pos\nDEBUG:matplotlib.axes._base:update_title_pos\n\n\n\n\n\n\n\n\n\n## 12. Which movies are most similar?\n\nWe can now determine the similarity between movies based on their plots! To wrap up, let’s answer one final question: which movie is most similar to the movie Braveheart?\n\n\nbidx = movies_df.title.eq('Braveheart').idxmax()\n\n\nnp.argsort(similarity_distance[bidx])[]\n\narray([50, 34,  6,  0, 33, 57, 11, 48, 42, 15, 60, 36, 55, 35, 70, 44, 47,\n       62, 16,  3, 31, 30, 61, 58, 66, 72, 46,  2, 77, 19, 79, 73, 88, 53,\n       45,  4, 52, 10, 49, 86, 38, 18, 51, 87, 24, 74, 95, 65,  5, 28,  8,\n       64, 92, 29, 98, 71, 97, 39, 68, 90, 37, 32, 76, 82, 14, 56, 43, 23,\n        7, 59, 20, 80, 12, 27, 13, 94, 81, 75, 91, 84, 93, 83, 40, 22, 63,\n       54,  1, 21, 99, 85, 69, 89, 96, 78, 25,  9, 41, 67, 26, 17])\n\n\n\n movies_df.title.iloc[50]\n\n'Braveheart'\n\n\n\n# Answer the questionans = \"The Sound of Music\"print(ans)"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/international_debt/notebook.html",
    "href": "datacamp_projects/workspace/SQL/international_debt/notebook.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "Humans not only take debts to manage necessities. A country may also take debt to manage its economy. For example, infrastructure spending is one costly ingredient required for a country’s citizens to lead comfortable lives. The World Bank is the organization that provides debt to countries.In this project, you are going to analyze international debt data collected by The World Bank. The dataset contains information about the amount of debt (in USD) owed by developing countries across several categories. You are going to find the answers to the following questions:- What is the number of distinct countries present in the database?- What country has the highest amount of debt?- What country has the lowest amount of repayments?Below is a description of the table you will be working with:## international_debt table| Column | Definition | Data Type ||-|-|-||country_name|Name of the country|varchar||country_code|Code representing the country|varchar||indicator_name|Description of the debt indicator|varchar||indicator_code|Code representing the debt indicator|varchar||debt|Value of the debt indicator for the given country (in current US dollars)|float|You will execute SQL queries to answer three questions, as listed in the instructions.\n\n-- num_distinct_countries SELECT COUNT(DISTINCT country_name) as total_distinct_countriesFROM international_debt\n\n\n\n\n\n\n\n\ntotal_distinct_countries\n\n\n\n\n0\n124\n\n\n\n\n\n\n\n\n-- highest_debt_country SELECT country_name, SUM(debt) as total_debtFROM international_debtGROUP BY country_nameORDER BY total_debt DESCLIMIT 1\n\n\n\n\n\n\n\n\ncountry_name\ntotal_debt\n\n\n\n\n0\nChina\n2.857935e+11\n\n\n\n\n\n\n\n\n-- lowest_principal_repayment SELECT country_name, indicator_name, MAX(debt) as lowest_repaymentFROM international_debtWHERE indicator_code = 'DT.AMT.DLXF.CD'GROUP BY country_name, indicator_nameORDER BY lowest_repayment ASCLIMIT 1\n\n\n\n\n\n\n\n\ncountry_name\nindicator_name\nlowest_repayment\n\n\n\n\n0\nTimor-Leste\nPrincipal repayments on external debt, long-te...\n825000"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/london_transport/notebook.html",
    "href": "datacamp_projects/workspace/SQL/london_transport/notebook.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "London, or as the Romans called it “Londonium”! Home to over 8.5 million residents who speak over 300 languages. While the City of London is a little over one square mile (hence its nickname “The Square Mile”), Greater London has grown to encompass 32 boroughs spanning a total area of 606 square miles! Given the city’s roads were originally designed for horse and cart, this area and population growth has required the development of an efficient public transport system! Since the year 2000, this has been through the local government body called Transport for London, or TfL, which is managed by the London Mayor’s office. Their remit covers the London Underground, Overground, Docklands Light Railway (DLR), buses, trams, river services (clipper and Emirates Airline cable car), roads, and even taxis.The Mayor of London’s office make their data available to the public here. In this project, you will work with a slightly modified version of a dataset containing information about public transport journey volume by transport type. The data has been loaded into a Google BigQuery database called TFL with a single table called JOURNEYS, including the following data:## TFL.JOURNEYS| Column | Definition | Data type ||——–|————|———–|| MONTH| Month in number format, e.g., 1 equals January | INTEGER || YEAR | Year | INTEGER || DAYS | Number of days in the given month | INTEGER || REPORT_DATE | Date that the data was reported | DATE || JOURNEY_TYPE | Method of transport used | VARCHAR || JOURNEYS_MILLIONS | Millions of journeys, measured in decimals | FLOAT |Note that the table name is upper case* by default.You will execute SQL queries to answer three questions, as listed in the instructions.\n\n-- most_popular_transport_typesSELECT JOURNEY_TYPE, SUM(JOURNEYS_MILLIONS) as TOTAL_JOURNEYS_MILLIONSFROM TFL.JOURNEYSGROUP BY JOURNEY_TYPEORDER BY TOTAL_JOURNEYS_MILLIONS DESC;\n\n\n\n\n\n\n\n\nJOURNEY_TYPE\nTOTAL_JOURNEYS_MILLIONS\n\n\n\n\n0\nBus\n24905.193947\n\n\n1\nUnderground & DLR\n15020.466544\n\n\n2\nOverground\n1666.845666\n\n\n3\nTfL Rail\n411.313421\n\n\n4\nTram\n314.689875\n\n\n5\nEmirates Airline\n14.583718\n\n\n\n\n\n\n\n\n-- emirates_airline_popularitySELECT YEAR, MONTH, ROUND(SUM(JOURNEYS_MILLIONS),2) as ROUNDED_JOURNEYS_MILLIONSFROM TFL.JOURNEYSWHERE JOURNEY_TYPE = 'Emirates Airline' AND JOURNEY_TYPE IS NOT NULLGROUP BY YEAR, MONTH, JOURNEY_TYPEORDER BY ROUNDED_JOURNEYS_MILLIONS DESC, YEAR ASCLIMIT 5\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nROUNDED_JOURNEYS_MILLIONS\n\n\n\n\n0\n2012\n5\n0.53\n\n\n1\n2012\n6\n0.38\n\n\n2\n2012\n4\n0.24\n\n\n3\n2013\n5\n0.19\n\n\n4\n2015\n5\n0.19\n\n\n\n\n\n\n\n\n-- least_popular_years_tubeSELECT YEAR, JOURNEY_TYPE, SUM(JOURNEYS_MILLIONS) as TOTAL_JOURNEYS_MILLIONSFROM TFL.JOURNEYSWHERE JOURNEY_TYPE = 'Underground & DLR'GROUP BY YEAR, JOURNEY_TYPEORDER BY TOTAL_JOURNEYS_MILLIONS ASCLIMIT 5\n\n\n\n\n\n\n\n\nYEAR\nJOURNEY_TYPE\nTOTAL_JOURNEYS_MILLIONS\n\n\n\n\n0\n2020\nUnderground & DLR\n310.179316\n\n\n1\n2021\nUnderground & DLR\n748.452544\n\n\n2\n2022\nUnderground & DLR\n1064.859009\n\n\n3\n2010\nUnderground & DLR\n1096.145588\n\n\n4\n2011\nUnderground & DLR\n1156.647654"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html",
    "title": "Do students describe professors differently based on gender?",
    "section": "",
    "text": "Note: You can consult the solution of this live training in the file browser as notebook-solution.ipynb\nLanguage plays a crucial role in shaping our perceptions and attitudes towards gender in the workplace, in classrooms, and personal relationships. Studies have shown that gender bias in language can have a significant impact on the way people are perceived and treated.\nFor example, research has found that job advertisements that use masculine-coded language tend to attract more male applicants, while those that use feminine-coded language tend to attract more female applicants. Similarly, gendered language can perpetuate differences in the classroom.\nIn this project, we’ll using scraped student reviews from ratemyprofessors.com to identify differences in language commonly used for male vs. female professors, and explore subtleties in how language in the classroom can be gendered.\nThis excellent tool created by Ben Schmidt allows us to enter the words and phrases that we find in our analysis and explore them in more depth. We’ll do this at the end.\nCatalyst also does some incredible work on decoding gendered language."
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "title": "Do students describe professors differently based on gender?",
    "section": "3a. What additional package imports are required for data visualization and NLP?",
    "text": "3a. What additional package imports are required for data visualization and NLP?\n::: {#8d6a7a01-887d-4df1-98b6-5a3c95e35519 .cell executionCancelledAt=‘null’ executionTime=‘1184’ jupyter=‘{“outputs_hidden”:false,“source_hidden”:false}’ lastExecutedAt=‘1719766570161’ lastExecutedByKernel=‘f6c43d9d-c556-42d8-9544-e49e4eaa9fbc’ lastScheduledRunId=‘null’ lastSuccessfullyExecutedCode=’import numpy as np # For manipulating matrices during NLP\nimport nltk # Natural language toolkit from nltk.tokenize import word_tokenize # Used for breaking up strings of text (e.g. sentences) into words from nltk.stem.porter import PorterStemmer # Used to return the dictionary base of a word from nltk.tokenize import WhitespaceTokenizer # Used for breaking up strings of text (e.g. sentences) into words based on white space\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # Used to count the occurences of words and phrases from sklearn.feature_extraction import text # Using to extrat features from text"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "title": "Do students describe professors differently based on gender?",
    "section": "Task 3d. What are the most important words being used to describe professors in reviews?",
    "text": "Task 3d. What are the most important words being used to describe professors in reviews?\nLet’s write a custom function that tokenizes and lemmatizes our list of words. - Word tokenization: process of splitting text into individual words, called tokens. A common preprocessing step in natural language processing (NLP) so that text can be analyzed and processed more easily. Methods include whitespace tokenization, regular expression-based tokenization, and rule-based tokenization. We’ll be using the word_tokenize tokenizer from nltk, with all its defaults. - Lemmatization: process of reducing words to their base or dictionary form, called the lemma. Also a common pre-processing step in NLP, so that words with a common base form are treated the same way. For example, the lemma of “am” is “be”, of “running” is “run”, and of “mice” is “mouse”.\n\nPorterStemmer().stem(\"she\\'s\")\n\n\"she'\"\n\n\n\nword_tokenize('she\\'s a girl')\n\n['she', \"'s\", 'a', 'girl']\n\n\n\nimport string\n\n\ndef tokenize(text):\n    tk = WhitespaceTokenizer()\n    tokens = tk.tokenize(text)\n    stems = []\n    for item in tokens:\n        stems.append(PorterStemmer().stem(item).strip(string.punctuation))\n    return stems\n\nLet’s import a list of stop words, which are common English words that we will be ignoring in our analysis. sklearn provides a common list of stop words, and we can append additional words to this list. Below, we append pronouns, along with the words “class” and “student”. Feel free to add any additional words you’d like to ignore to this list later on as you try to build upon this analysis!\n\nmy_stop_words = sktext.ENGLISH_STOP_WORDS.union([\"he\",\"she\",\"his\",\"her\",\n                                              \"himself\",\"herself\", \"hers\",\"shes\"\n                                              \"class\",\"student\", 'man', 'woman', 'girl',\n                                                 'guy', 'lady', 'mr', 'mrs', 'ms'])\nmy_stop_words = my_stop_words.union([tokenize(word)[0] for word in my_stop_words])\n\nFor the purpose of analyzing review texts, we want to move from having one row for each professor to one row for each review. Lets do this with .explode() from pandas.\n\ndf_quality = df[(df['review'].apply(len) == df['quality'].apply(len))]\nq = df_quality[['pronouns','review','quality']].explode(['review','quality'], ignore_index=True).dropna()\nq['quality'] = q['quality'].astype(float)\n\n\nq.head(5)\n\n\n\n\n\n\n\n\npronouns\nreview\nquality\n\n\n\n\n0\nF\nGood experience for a class online. It was unc...\n4.0\n\n\n1\nF\nHonestly she didnt teach good at all and she w...\n2.0\n\n\n2\nF\nI think if you go by word for word in the modu...\n1.0\n\n\n3\nF\nTook her online class CSS64. We started buildi...\n1.0\n\n\n4\nF\nTook her for a late start hybrid class (Bus43)...\n3.0\n\n\n\n\n\n\n\nTFIDF vectorization is the process of assigning scores to each review in a document based on how frequently the word occurs, normalized by how frequently the word occurs in the dataset overall.\nWe’ll use TfidfVectorizer() to generate these scores. This will return a matrix, with as many rows as reviews, and as many columns as words in our dataset.\n\nvec = TfidfVectorizer(\n    tokenizer = tokenize,\n    stop_words = list(my_stop_words),\n    ngram_range = (1,4)\n)\nX = vec.fit_transform(q.review)\nfeature_names = vec.get_feature_names_out()\n\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n/home/kantundpeterpan/miniconda3/envs/nlp/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anywh', 'becau', 'el', 'elsewh', 'everywh', 'ind', 'otherwi', 'plea', 'somewh'] not in stop_words.\n  warnings.warn(\n\n\n\nfeature_names.shape\n\n(365516,)\n\n\n\nnp.random.choice(feature_names, size = 10)\n\narray([\"sense you'v\", 'work easy week class',\n       'technology(blackboard),noon lac know', 'just read quizz',\n       \"can't drop day\", 'lot question understand materi', 'class ad',\n       'languag pain', 'style rel scatter unorganized',\n       \"prof l'heureux help\"], dtype=object)\n\n\nX is a sparse matrix. We’ll now move into filtering X for: - Rows with male professors and reviews of high quality - Rows with female professors and reviews of high quality - Rows with male professors and reviews of low quality - Rows with female professors and reviews of low quality\nWe can explore feature importance in each of these to get a sense of which words and phrases are coming up most often in the data.\n\nm_pos = X[q.pronouns.eq('M') & q.quality.ge(4.5)]\n\n\nm_pos = X[q.pronouns.eq('M') & q.quality.ge(4.5)]\nf_pos = X[q.pronouns.eq('F') & q.quality.ge(4.5)]\nm_neg = X[q.pronouns.eq('M') & q.quality.le(2.5)]\nf_neg = X[q.pronouns.eq('F') & q.quality.le(2.5)]\n\n\nnp.unique(np.array(m_pos[0,:].todense()))\n\narray([0.        , 0.04840186, 0.05048048, 0.05840671, 0.0588915 ,\n       0.06440657, 0.07216968, 0.07743822, 0.08628339, 0.0890904 ,\n       0.09454997, 0.09609708, 0.0982585 , 0.09872669, 0.10101388,\n       0.10570444, 0.11337177, 0.12547472, 0.13281775, 0.13699447,\n       0.146295  , 0.15107613, 0.15781475])\n\n\nLet’s have a look at what language students are using to describe male professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(m_pos.sum(axis = 0)))[0,::-1]\nm_pos_features = feature_names[importance[:300]]\n\nPrint out the 25 most important features\n\nm_pos_features[:25]\n\narray(['comment', 'great', 'class', 'teacher', 'best', 'professor',\n       'prof', 'good', 'help', 'realli', 'make', 'easi', 'love',\n       'great teacher', 'awesom', 'know', 'learn', '', 'work', 'lot',\n       'lectur', 'amaz', 'cours', 'nice', 'test'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively.\n\nimportance = np.argsort(np.asarray(f_pos.sum(axis = 0)))[0,::-1]\nf_pos_features = feature_names[importance[:300]]\nf_pos_features[:25]\n\narray(['comment', 'great', 'class', 'teacher', 'prof', 'help',\n       'professor', 'best', 'good', 'easi', 'realli', 'work', 'nice',\n       'lot', 'love', 'make', 'learn', 'helpful', 'great teacher', '',\n       'amaz', 'great prof', 'cours', 'lectur', 'hard'], dtype=object)\n\n\nIt should be interesting if there are words exclusively used for one gender\n\n#male\nonly_m_pos = ~np.in1d(m_pos_features, f_pos_features)\nm_pos_features[only_m_pos][:25]\n\n/tmp/ipykernel_832826/3886134522.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_m_pos = ~np.in1d(m_pos_features, f_pos_features)\n\n\narray(['fantast', 'excel teacher', 'topic', 'awesom professor',\n       'excel professor', 'u', 'brilliant', 'realli enjoy', '2', 'old',\n       'amaz professor', \"professor i'v\", \"teacher i'v\", 'mark', 'b',\n       'hot', 'review', 'genuin', 'bore', \"he'll\", 'reason', 'hours',\n       'want learn', 'overal', 'feel'], dtype=object)\n\n\n\n#female\nonly_f_pos = ~np.isin(f_pos_features, m_pos_features)\nf_pos_features[only_f_pos][:25]\n\narray(['extra credit', 'especi', 'spanish', \"she'll\", 'assignments',\n       'realli nice', 'offer', 'help prof', 'class nice', 'succeed',\n       'realli want', 'alot', 'onlin class', 'knowledgable', 'group',\n       'good lectur', 'attention', 'nice help', \"i'd\", 'real world',\n       'easi grade', 'awsom', 'account', 'guid', 'semester'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe male professors negatively.\n\nimportance = np.argsort(np.asarray(m_neg.sum(axis = 0)))[0,::-1]\nm_neg_features = feature_names[importance[:300]]\nm_neg_features[:25]\n\narray(['comment', 'class', 'teach', 'hard', 'test', 'worst', 'professor',\n       'teacher', 'lectur', '', 'time', 'know', 'like', 'grade', \"don't\",\n       \"doesn't\", 'just', 'prof', 'avoid', 'bore', 'question', 'good',\n       'make', 'doe', 'read'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors negatively.\n\nimportance = np.argsort(np.asarray(f_neg.sum(axis = 0)))[0,::-1]\nf_neg_features = feature_names[importance[:300]]\nf_neg_features[:25]\n\narray(['comment', 'class', 'worst', 'grade', 'teacher', 'hard', 'teach',\n       \"don't\", \"doesn't\", 'help', 'like', 'test', '', 'just', 'time',\n       'work', 'professor', 'good', 'doe', 'question', 'make', 'know',\n       'horribl', 'learn', 'unclear'], dtype=object)\n\n\nSame analysis for exclusive words:\n\n#male\nonly_m_neg = ~np.in1d(m_neg_features, f_neg_features)\nm_neg_features[only_m_neg][:25]\n\n/tmp/ipykernel_832826/728473888.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_m_neg = ~np.in1d(m_neg_features, f_neg_features)\n\n\narray(['speak', 'hard understand', 'great', 'arrog', 'hear', 'costs',\n       'smart', 'hardest', 'taught', 'listen', 'rambl', 'english',\n       'exampl', 'let', 'sit', 'incred', 'wrote', 'knowledg', 'possible',\n       'probabl', 'avoid costs', 'gpa', 'offic', '1', \"can't teach\"],\n      dtype=object)\n\n\n\n#female\nonly_f_neg = ~np.in1d(f_neg_features, m_neg_features)\nf_neg_features[only_f_neg][:25]\n\n/tmp/ipykernel_832826/1584707764.py:2: DeprecationWarning: `in1d` is deprecated. Use `np.isin` instead.\n  only_f_neg = ~np.in1d(f_neg_features, m_neg_features)\n\n\narray(['late', 'disorgan', 'gave', 'advis', 'annoy', 'feedback', 'cost',\n       'agre', 'helpful', 'disorganized', 'nice person', 'avoid cost',\n       'unorganized', 'opinion', 'slow', 'quit', 'colleg', 'honestli',\n       'fan', 'papers', 'spanish', 'easi class', 'favorites', 'instead',\n       '5'], dtype=object)"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#congratulations-on-making-it-to-the-end",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook.html#congratulations-on-making-it-to-the-end",
    "title": "Do students describe professors differently based on gender?",
    "section": "Congratulations on making it to the end!",
    "text": "Congratulations on making it to the end!\n\nWhere to from here?\n\nWe can feed these words into Ben Schmidt’s tool to derive insights by field.\nIf you’re interested in learning more about web scraping, take our courses on Web Scraping in Python\nIf you’re intersted in diving in to the world of Natural Language Processing, explore our skill track.\n\n::: {#f592331c-f5d8-4c32-8791-2eee74269815 .cell executionCancelledAt=‘null’ executionTime=‘48’ jupyter=‘{“source_hidden”:true}’ lastExecutedAt=‘1719766585577’ lastExecutedByKernel=‘f6c43d9d-c556-42d8-9544-e49e4eaa9fbc’ lastScheduledRunId=‘null’ lastSuccessfullyExecutedCode=’import os from IPython.display import display, HTML"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/web_scraping.html",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/web_scraping.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "This code won’t run in wowrkspace, but you can download it locally if you are interested in learning about how we obtained the URLs of the professors used in this project.\n\n!pip install selenium!pip install webdriver-manager\n\n\nfrom selenium import webdriverfrom selenium.webdriver.chrome.service import Servicefrom webdriver_manager.chrome import ChromeDriverManagerfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECimport requestsfrom bs4 import BeautifulSoupfrom lxml import etreefrom urllib.request import urlopen\n\n\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))driver.get(\"https://www.ratemyprofessors.com/search/teachers?query=?\")# Wait for initialize, in secondswait = WebDriverWait(driver, 8)wait.until(EC.visibility_of_element_located((By.XPATH, '/html/body/div[5]/div/div/button')))button = driver.find_element('xpath','/html/body/div[5]/div/div/button')button.click()clicks = 0while clicks &lt; 140:    if clicks%10==0:        print(f'Clicked {clicks} times.')    wait = WebDriverWait(driver, 7)    time.sleep(3)    wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[4]/button')))    show_more = driver.find_element('xpath','//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[4]/button')    show_more.click()    clicks += 1cards = driver.find_elements(By.XPATH,'//*[@id=\"root\"]/div/div/div[4]/div[1]/div[1]/div[3]/a[*]')driver.quit()profs = [i.get_attribute('href') for i in cards]"
  },
  {
    "objectID": "guided.html",
    "href": "guided.html",
    "title": "Datacamp guided projects and code-alongs",
    "section": "",
    "text": "Can I give some explanation here ?\n\n\n\n\n\n\n\n\n\n\n\nImport and observe dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo students describe professors differently based on gender?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo students describe professors differently based on gender?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTFL.JOURNEYS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrump vs. Trudeau: Tweet classification\n\n\n\n\n\n\nDatacamp & Heiner Atze\n\n\n\n\n\n\n\n\n\n\n\n\ninternational_debt table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncharging_sessions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\nNo matching items\n\n:::\n:::",
    "crumbs": [
      "Datacamp",
      "Guided"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html",
    "title": "Do students describe professors differently based on gender?",
    "section": "",
    "text": "Language plays a crucial role in shaping our perceptions and attitudes towards gender in the workplace, in classrooms, and personal relationships. Studies have shown that gender bias in language can have a significant impact on the way people are perceived and treated. For example, research has found that job advertisements that use masculine-coded language tend to attract more male applicants, while those that use feminine-coded language tend to attract more female applicants. Similarly, gendered language can perpetuate differences in the classroom.In this project, we’ll using scraped student reviews from ratemyprofessors.com to identify differences in language commonly used for male vs. female professors, and explore subtleties in how language in the classroom can be gendered.This excellent tool created by Ben Schmidt allows us to enter the words and phrases that we find in our analysis and explore them in more depth. We’ll do this at the end.Catalyst also does some incredible work on decoding gendered language."
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#a.-what-additional-package-imports-are-required-for-data-visualization-and-nlp",
    "title": "Do students describe professors differently based on gender?",
    "section": "3a. What additional package imports are required for data visualization and NLP?",
    "text": "3a. What additional package imports are required for data visualization and NLP?\n::: {#8d6a7a01-887d-4df1-98b6-5a3c95e35519 .cell jupyter=‘{“outputs_hidden”:false,“source_hidden”:false}’ executionTime=‘841’ lastSuccessfullyExecutedCode=’import numpy as np # For manipulating matrices during NLP\nimport nltk # Natural language toolkit from nltk.tokenize import word_tokenize # Used for breaking up strings of text (e.g. sentences) into words from nltk.stem.porter import PorterStemmer # Used to return the dictionary base of a word from nltk.tokenize import WhitespaceTokenizer # Used for breaking up strings of text (e.g. sentences) into words based on white space\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # Used to count the occurences of words and phrases from sklearn.feature_extraction import text # Using to extrat features from text"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#task-3d.-what-are-the-most-important-words-being-used-to-describe-professors-in-reviews",
    "title": "Do students describe professors differently based on gender?",
    "section": "Task 3d. What are the most important words being used to describe professors in reviews?",
    "text": "Task 3d. What are the most important words being used to describe professors in reviews?\nLet’s write a custom function that tokenizes and lemmatizes our list of words.- Word tokenization: process of splitting text into individual words, called tokens. A common preprocessing step in natural language processing (NLP) so that text can be analyzed and processed more easily. Methods include whitespace tokenization, regular expression-based tokenization, and rule-based tokenization. We’ll be using the word_tokenize tokenizer from nltk, with all its defaults.- Lemmatization: process of reducing words to their base or dictionary form, called the lemma. Also a common pre-processing step in NLP, so that words with a common base form are treated the same way. For example, the lemma of “am” is “be”, of “running” is “run”, and of “mice” is “mouse”.\n\ndef tokenize(text):    tk = WhitespaceTokenizer()    tokens = tk.tokenize(text)    stems = []    for item in tokens:        stems.append(PorterStemmer().stem(item))    return stems    # return tokens\n\nLet’s import a list of stop words, which are common English words that we will be ignoring in our analysis. sklearn provides a common list of stop words, and we can append additional words to this list. Below, we append pronouns, along with the words “class” and “student”.\n\nmy_stop_words = text.ENGLISH_STOP_WORDS.union([\"he\",\"she\",\"his\",\"her\",                                              \"himself\",\"herself\", \"hers\",\"shes\"                                              \"class\",\"student\"])\n\nFor the purpose of analyzing review texts, we want to move from having one row for each professor to one row for each review. Lets do this with .explode() from pandas.\n\ndf_quality = df[(df['review'].apply(len) == df['quality'].apply(len))]q = df_quality[['pronouns','review','quality']].explode(['review','quality'], ignore_index=True).dropna()q['quality'] = q['quality'].astype(float)\n\nTFIDF vectorization is the process of assigning scores to each review in a document based on how frequently the word occurs, normalized by how frequently the word occurs in the dataset overall.We’ll use TfidfVectorizer() to generate these scores. This will return a matrix, with as many rows as reviews, and as many columns as words in our dataset.\n\nvec = TfidfVectorizer(tokenizer=tokenize, stop_words=my_stop_words,                     ngram_range=(1,4))X = vec.fit_transform(q['review'])feature_names = vec.get_feature_names_out()\n\nX is a sparse matrix. We’ll now move into filtering X for:- Male professors only- Female professors only- Rows with male professors and reviews of high quality - Rows with female professors and reviews of high quality - Rows with male professors and reviews of low quality - Rows with female professors and reviews of low quality We can explore feature importance in each of these to get a sense of which words and phrases are coming up most often in the data.\n\nm_pos = X[(q['pronouns']=='M') & (q['quality']&gt;=4.5),:] f_pos = X[(q['pronouns']=='F') & (q['quality']&gt;=4.5),:] m_neg = X[(q['pronouns']=='M') & (q['quality']&lt;2.5),:] f_neg = X[(q['pronouns']=='M') & (q['quality']&lt;2.5),:] \n\nLet’s have a look at what language students are using to describe male professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(m_pos.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'great', 'hi', 'veri', 'class', 'best', 'good',\n       'professor', 'realli', 'teacher', \"he'\", 'wa', 'thi', 'make',\n       'help', 'easi', 'love', 'prof', 'awesom', 'know', 'class.',\n       'learn', 'lectur', 'amaz', 'cours', 'excel', 'just', 'test',\n       'alway', 'prof.', 'lot', 'work', 'nice', 'ha', \"i'v\", 'teach',\n       'teacher.', 'hi class', 'want', 'best professor', 'him.',\n       'professor.', 'like', 'guy', 'class,', 'dr.', 'funni', 'hard',\n       'need', 'read', 'know hi', 'fun', 'clear', 'materi', 'enjoy',\n       'great teacher', 'recommend', 'studi', 'exam', 'care', 'note',\n       'guy.', 'time', 'best prof', 'understand', 'best teacher',\n       'teacher,', 'everi', 'definit', \"don't\", \"you'll\", 'it.', 'onli',\n       'everyth', 'man', 'becaus', 'ani', 'fair', 'took', 'students.',\n       'extrem', 'veri good', 'use', 'had.', 'knowledg', 'explain',\n       'prof!', 'interesting.', 'well.', 'grade', 'veri help', 'question',\n       'fantast', 'math', 'book', 'thi class', 'hi lectur', 'write',\n       'highli', 'think', 'doe', 'attend', 'actual', 'professor,',\n       'thing', 'pretti', 'stuff', 'assign', 'prof,', 'hi students.',\n       'got', 'helpful.', 'befor', 'favorit', 'peopl', 'sure', 'him!',\n       'way', 'care hi', \"it'\", 'long', 'talk', 'absolut', 'super',\n       'guy,', '-', 'passion', 'littl', 'funny,', 'tough', 'hi class.',\n       'cool', '&', 'tri', 'professor!', 'great teacher,', 'wish', 'tell',\n       'person', 'a.', 'good teacher', 'helpful,', 'final', 'make sure',\n       'year', 'expect', 'difficult', 'make class', 'come', 'ask',\n       'love thi', 'subject', 'everyon', 'teacher!', 'man.', 'hi test',\n       'look', 'u', 'material.', 'answer', 'listen', 'sens', 'pay',\n       'class wa', 'great prof.', 'paper', 'offic', 'great professor.',\n       'wa veri', 'worth', 'say', 'far', 'histori', \"you'r\", 'hi stuff',\n       'great teacher.', 'great prof!', 'know hi stuff', 'interesting,',\n       'veri easi', 'easy.', 'bit', 'learn lot', 'veri nice', \"doesn't\",\n       'veri clear', 'classes.', 'thought', \"he' veri\", 'best.', 'smart',\n       'fine.', 'anyon', \"i'v had.\", 'realli enjoy', 'onlin', 'you.',\n       'love hi', 'prepar', 'thi class.', 'stori', 'essay', 'exampl',\n       'entertain', 'attent', 'better', 'course.', 'mani', '2',\n       'homework', 'taken', 'engag', 'bad', 'great prof,', 'notes.',\n       'textbook', 'truli', 'realli know', 'awesome.', 'real',\n       'excel teacher.', \"didn't\", 'inform', 'miss', 'challeng', 'review',\n       'old', 'class!', \"professor i'v\", 'thi guy', 'problem', 'let',\n       'genuin', 'great!', 'too.', 'did', 'work.', 'topic', 'great guy.',\n       \"he'll\", 'best!', 'enjoy hi', 'brilliant', 'veri knowledg',\n       'concept', 'overal', 'bore', \"i'm\", 'dure', \"teacher i'v\",\n       'alway help', 'approach', 'stuff.', 'feel', 'thi professor',\n       'pay attent', 'intellig', 'mark', 'effort', 'midterm', 'great.',\n       'extra', 'tests.', 'veri helpful.', 'instructor', 'text', 'reason',\n       'hi stuff.', 'taught', 'quizz', 'goe', 'lab', 'end', 'teacher!!',\n       'easy,', ':)', 'lot.', 'great professor', 'know hi stuff.',\n       'probabl', 'grade.', 'quit', 'pass', 'exams.', 'fair.', 'kind',\n       'hand', 'incred', 'avail', 'realli know hi', 'highli recommend',\n       \"can't\", 'wonder', 'humor', 'lectures.', 'job', 'him,',\n       'great prof', 'major', 'hour'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively. The code below will return the 300 most important ngrams.\n\nimportance = np.argsort(np.asarray(f_pos.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'veri', 'great', 'class', 'help', \"she'\", 'easi',\n       'realli', 'best', 'wa', 'good', 'professor', 'thi', 'teacher',\n       'make', 'love', 'lot', 'prof', 'class.', 'work', 'learn', 'prof.',\n       'nice', 'lectur', 'cours', 'like', 'amaz', 'want', 'excel',\n       'teacher.', 'care', 'know', 'her.', 'alway', 'ha', 'wonder',\n       'just', 'extrem', 'hard', 'awesom', 'recommend', 'dr.', 'need',\n       'test', 'fair', 'time', 'understand', 'professor.', \"don't\",\n       'studi', 'best teacher', 'teach', 'her!', 'read', 'exam', 'grade',\n       'assign', 'class,', 'veri help', 'highli', 'thing',\n       'best professor', 'helpful.', 'materi', 'clear', 'interesting.',\n       'super', \"it'\", \"i'v\", 'onlin', 'great.', 'awesome!', 'enjoy',\n       'extra', 'sure', 'ani', 'thi class', 'definit', 'students.',\n       'pretti', 'becaus', 'question', 'ladi', 'pay', 'everi', 'her,',\n       'talk', 'mrs.', 'veri nice', 'actual', \"you'll\", 'work,',\n       'passion', 'took', 'teacher!', 'veri good', 'homework',\n       \"she' veri\", 'everyth', 'fun', 'class!', 'best prof', 'professor,',\n       'well.', 'come', 'onli', 'better', '-', 'think', 'great prof.',\n       'explain', 'wa veri', 'real', 'highli recommend', 'doe', 'tough',\n       'wish', 'world', 'a.', 'excel prof', 'long', 'use', 'littl',\n       'person', 'material.', 'attent', 'great teacher.', 'sweet',\n       'veri easi', 'peopl', 'prof,', 'helpful,', 'credit', 'had.', '&',\n       'it.', 'anyth', 'great!', 'lab', 'knowledg', 'expect', 'prof!',\n       'problem', 'write', 'professor!', 'teacher,', \"you'r\", 'approach',\n       'look', 'math', 'pass', 'thi cours', 'pay attent', 'discuss',\n       'note', 'you.', 'absolut', 'class wa', 'course.', 'mani', 'ever!',\n       'befor', 'extra credit', 'sens', 'especi', 'taught', 'person.',\n       'got', 'learn lot', 'help.', 'bit', 'work.',\n       'best professor ever!', 'understand.', 'classes.', 'fair.',\n       'lectures.', 'ask', 'great prof', 'nice,', 'grade.', 'tell',\n       'year', 'lady.', 'professor ever!', 'point', 'thought', 'attend',\n       'cool', 'quizz', 'make sure', 'way', 'veri helpful.', 'answer',\n       'students,', 'fine.', \"she'll\", 'easi understand.', 'amazing.',\n       'did', 'end', 'instructor', 'great teacher', 'project', 'challeng',\n       'respect', 'offer', 'great professor!', 'entertain', 'essay',\n       'good lectur', 'veri clear', 'favorit', 'difficult', 'english',\n       'exampl', 'anyon', 'truli', 'great prof!', 'great teacher,', 'goe',\n       'realli want', \"i'm\", 'nice lady.', 'intellig', 'classes!',\n       'awsom', 'concept', 'veri helpful,', 'lot.', 'book', \"doesn't\",\n       'group', 'lectures,', 'them.', 'listen', 'kind', 'smart', '3',\n       'funni', \"i'd\", 'easy.', 'veri fair', 'great professor,', 'tri',\n       \"she' realli\", 'make class', 'open', 'guid', 'recommend her.',\n       'inspir', 'spanish', 'account', 'probabl', 'offic', 'easi grade!',\n       'paper', 'school', 'woman', 'great prof,', 'great teacher!',\n       'questions.', 'taken', 'help prof', 'real world', 'love her!',\n       'stori', 'good professor.', 'let', \"isn't\", 'effort', \"prof i'v\",\n       'worth', 'alot', 'avail', 'funny.', 'subject', 'book.', 'hour',\n       'job', 'too.', 'yummer', 'best!!', 'ã\\x82â', 'knowledgable.',\n       'research', 'say', 'clearli', 'engag', 'realli know', 'helpful!',\n       'requir', 'prepar', 'demand', 'exams.', 'midterm', 'thank', 'far',\n       'great professor.', 'easy,'], dtype=object)\n\n\nLet’s have a look at what language students are using to describe male professors negatively.\n\nimportance = np.argsort(np.asarray(m_neg.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'hi', 'thi', 'class', 'veri', 'wa', 'teach', 'worst',\n       'test', 'lectur', \"doesn't\", 'class.', 'like', 'hard', 'just',\n       \"don't\", 'professor', 'doe', 'know', 'grade', 'make', 'avoid',\n       'bore', 'teacher', \"he'\", 'question', 'onli', 'terribl', 'good',\n       'read', 'horribl', 'ha', 'learn', 'cours', 'prof', 'time', 'exam',\n       'understand', 'thi class', 'talk', 'becaus', 'help', 'realli',\n       'guy', 'him.', 'explain', 'bad', 'say', 'ask', 'ani', 'hi class',\n       \"can't\", 'thing', 'think', \"didn't\", 'way', 'book', 'thi guy',\n       'noth', 'need', 'materi', 'answer', 'wast', 'hi lectur', 'extrem',\n       'work', 'everi', 'want', 'expect', 'nice', 'teacher.', 'class,',\n       'assign', 'worst professor', 'all.', 'studi', 'professor.', 'did',\n       'note', 'anyth', 'use', \"i'v\", 'difficult', 'final', 'unclear',\n       'tell', 'drop', 'hi test', 'complet', 'worst prof', 'confus',\n       'dont', 'man', 'tri', 'absolut', 'got', 'time.', 'everyth', '-',\n       'easi', 'had.', 'rude', 'it.', 'write', 'fail', 'stay',\n       'worst teacher', '3', 'point', 'math', 'hard.', 'class wa', 'lot',\n       'mark', \"it'\", 'day', 'took', 'students.', 'clear', 'anoth',\n       'homework', 'tests.', 'useless', 'recommend', 'awful.', 'peopl',\n       'goe', 'littl', 'entir', 'unless', 'word', 'hi class.', 'someon',\n       'actual', 'come', 'textbook', 'hour', 'boring.', 'thi professor',\n       'lectures.', 'problem', 'dure', 'attend', 'better', 'half', 'went',\n       'veri hard', 'aw', 'care', 'pass', 'doesnt', 'prepar', 'everyon',\n       'person', 'paper', 'course.', 'ever.', 'look', '2', 'whi', 'mean',\n       'feel', 'year', 'arrog', 'midterm', 'prof.', 'ask question',\n       'thi class.', 'u', 'alway', \"i'm\", 'pretti', \"you'r\", 'hardest',\n       'speak', 'hate', 'imposs', 'old', 'befor', '&', 'end', 'thi cours',\n       'poor', 'subject', 'semest', 'said', 'told', 'guy,', 'someth',\n       'test.', 'basic', 'grade.', 'project', 'thi man', \"isn't\", 'mani',\n       'onlin', 'rambl', 'listen', 'idea', 'sit', 'base', 'differ',\n       'tough', 'incred', 'taught', 'essay', 'him,', 'away', 'hear',\n       'major', 'you.', \"won't\", 'terrible.', 'school', 'total', 'random',\n       'refus', 'concept', 'teach.', 'high', 'sure', 'exampl', 'slide',\n       'probabl', \"you'll\", 'anything.', 'veri difficult', 'week',\n       'hi teach', 'averag', 'questions.', 'follow', 'smart', 'save',\n       \"i'v had.\", 'possibl', 'avoid thi', 'right', 'life', 'wrote',\n       'far', 'boring,', 'bore lectur', 'topic', 'offic', 'knowledg',\n       'long', 'present', 'costs.', 'hi note', 'dr.', \"he'll\",\n       'answer question', 'believ', 'worth', 'pleas', \"don't know\", 'els',\n       'lab', '10', 'wrong.', 'run', 'harder', 'material.', 'wast time.',\n       'teacher,', \"doesn't explain\", 'let', 'nice guy', 'again.',\n       'veri bore', 'definit', 'quizz', 'chang', 'email', 'teaching.',\n       'them.', \"can't teach\", 'hi exam', 'minut', 'lecture.',\n       'thi teacher', 'pick', 'veri unclear', 'taken', 'avoid costs.',\n       'hard,', 'theori', 'book.', 'life.', \"wasn't\", 'suck', 'guy.',\n       ':(', \"worst professor i'v\", 'text', 'prof ever.', '....', 'group'],\n      dtype=object)\n\n\nLet’s have a look at what language students are using to describe female professors positively.\n\nimportance = np.argsort(np.asarray(f_neg.sum(axis=0)).ravel())[::-1]tfidf_feature_names = np.array(feature_names)tfidf_feature_names[importance[:300]]\n\narray(['comment', 'hi', 'thi', 'class', 'veri', 'wa', 'teach', 'worst',\n       'test', 'lectur', \"doesn't\", 'class.', 'like', 'hard', 'just',\n       \"don't\", 'professor', 'doe', 'know', 'grade', 'make', 'avoid',\n       'bore', 'teacher', \"he'\", 'question', 'onli', 'terribl', 'good',\n       'read', 'horribl', 'ha', 'learn', 'cours', 'prof', 'time', 'exam',\n       'understand', 'thi class', 'talk', 'becaus', 'help', 'realli',\n       'guy', 'him.', 'explain', 'bad', 'say', 'ask', 'ani', 'hi class',\n       \"can't\", 'thing', 'think', \"didn't\", 'way', 'book', 'thi guy',\n       'noth', 'need', 'materi', 'answer', 'wast', 'hi lectur', 'extrem',\n       'work', 'everi', 'want', 'expect', 'nice', 'teacher.', 'class,',\n       'assign', 'worst professor', 'all.', 'studi', 'professor.', 'did',\n       'note', 'anyth', 'use', \"i'v\", 'difficult', 'final', 'unclear',\n       'tell', 'drop', 'hi test', 'complet', 'worst prof', 'confus',\n       'dont', 'man', 'tri', 'absolut', 'got', 'time.', 'everyth', '-',\n       'easi', 'had.', 'rude', 'it.', 'write', 'fail', 'stay',\n       'worst teacher', '3', 'point', 'math', 'hard.', 'class wa', 'lot',\n       'mark', \"it'\", 'day', 'took', 'students.', 'clear', 'anoth',\n       'homework', 'tests.', 'useless', 'recommend', 'awful.', 'peopl',\n       'goe', 'littl', 'entir', 'unless', 'word', 'hi class.', 'someon',\n       'actual', 'come', 'textbook', 'hour', 'boring.', 'thi professor',\n       'lectures.', 'problem', 'dure', 'attend', 'better', 'half', 'went',\n       'veri hard', 'aw', 'care', 'pass', 'doesnt', 'prepar', 'everyon',\n       'person', 'paper', 'course.', 'ever.', 'look', '2', 'whi', 'mean',\n       'feel', 'year', 'arrog', 'midterm', 'prof.', 'ask question',\n       'thi class.', 'u', 'alway', \"i'm\", 'pretti', \"you'r\", 'hardest',\n       'speak', 'hate', 'imposs', 'old', 'befor', '&', 'end', 'thi cours',\n       'poor', 'subject', 'semest', 'said', 'told', 'guy,', 'someth',\n       'test.', 'basic', 'grade.', 'project', 'thi man', \"isn't\", 'mani',\n       'onlin', 'rambl', 'listen', 'idea', 'sit', 'base', 'differ',\n       'tough', 'incred', 'taught', 'essay', 'him,', 'away', 'hear',\n       'major', 'you.', \"won't\", 'terrible.', 'school', 'total', 'random',\n       'refus', 'concept', 'teach.', 'high', 'sure', 'exampl', 'slide',\n       'probabl', \"you'll\", 'anything.', 'veri difficult', 'week',\n       'hi teach', 'averag', 'questions.', 'follow', 'smart', 'save',\n       \"i'v had.\", 'possibl', 'avoid thi', 'right', 'life', 'wrote',\n       'far', 'boring,', 'bore lectur', 'topic', 'offic', 'knowledg',\n       'long', 'present', 'costs.', 'hi note', 'dr.', \"he'll\",\n       'answer question', 'believ', 'worth', 'pleas', \"don't know\", 'els',\n       'lab', '10', 'wrong.', 'run', 'harder', 'material.', 'wast time.',\n       'teacher,', \"doesn't explain\", 'let', 'nice guy', 'again.',\n       'veri bore', 'definit', 'quizz', 'chang', 'email', 'teaching.',\n       'them.', \"can't teach\", 'hi exam', 'minut', 'lecture.',\n       'thi teacher', 'pick', 'veri unclear', 'taken', 'avoid costs.',\n       'hard,', 'theori', 'book.', 'life.', \"wasn't\", 'suck', 'guy.',\n       ':(', \"worst professor i'v\", 'text', 'prof ever.', '....', 'group'],\n      dtype=object)"
  },
  {
    "objectID": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#congratulations-on-making-it-to-the-end-where-to-from-here--we-can-feed-these-words-into-ben-schmidts-tool-to-derive-insights-by-field.--if-youre-interested-in-learning-more-about-web-scraping-take-our-courses-on-web-scraping-in-python--if-youre-intersted-in-diving-in-to-the-world-of-natural-language-processing-explore-our-skill-track.",
    "href": "datacamp_projects/workspace/Python/NLP_gender_reviews/notebook-solution.html#congratulations-on-making-it-to-the-end-where-to-from-here--we-can-feed-these-words-into-ben-schmidts-tool-to-derive-insights-by-field.--if-youre-interested-in-learning-more-about-web-scraping-take-our-courses-on-web-scraping-in-python--if-youre-intersted-in-diving-in-to-the-world-of-natural-language-processing-explore-our-skill-track.",
    "title": "Do students describe professors differently based on gender?",
    "section": "Congratulations on making it to the end! ### Where to from here?- We can feed these words into Ben Schmidt’s tool to derive insights by field.- If you’re interested in learning more about web scraping, take our courses on Web Scraping in Python- If you’re intersted in diving in to the world of Natural Language Processing, explore our skill track.",
    "text": "Congratulations on making it to the end! ### Where to from here?- We can feed these words into Ben Schmidt’s tool to derive insights by field.- If you’re interested in learning more about web scraping, take our courses on Web Scraping in Python- If you’re intersted in diving in to the world of Natural Language Processing, explore our skill track."
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/ev_charging/notebook.html",
    "href": "datacamp_projects/workspace/SQL/ev_charging/notebook.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "As electronic vehicles (EVs) become more popular, there is an increasing need for access to charging stations, also known as ports. To that end, many modern apartment buildings have begun retrofitting their parking garages to include shared charging stations. A charging station is shared if it is accessible by anyone in the building.But with increasing demand comes competition for these ports — nothing is more frustrating than coming home to find no charging stations available! In this project, you will use a dataset to help apartment building managers better understand their tenants’ EV charging habits.The data has been loaded into a PostgreSQL database with a table named charging_sessions with the following columns:## charging_sessions| Column | Definition | Data type ||-|-|-||garage_id| Identifier for the garage/building|VARCHAR||user_id | Identifier for the individual user|VARCHAR||user_type|Indicating whether the station is Shared or Private| VARCHAR ||start_plugin|The date and time the session started |DATETIME||start_plugin_hour|The hour (in military time) that the session started | NUMERIC||end_plugout|The date and time the session ended | DATETIME ||eng_plugin_hour|The hour (in military time) that the session ended | NUMERIC||duration_hours| The length of the session, in hours|NUMERIC||el_kwh| Amount of electricity used (in Kilowatt hours)|NUMERIC||month_plugin| The month that the session started |VARCHAR||weekdays_plugin| The day of the week that the session started|VARCHAR|Let’s get started!#### Sources- Data: CC BY 4.0, via Kaggle,- Image: Julian Herzog, CC BY 4.0, via Wikimedia Commons\n\n-- unique_users_per_garage-- Modify the code belowSELECT garage_id, COUNT(DISTINCT user_id) as num_unique_usersFROM charging_sessionsWHERE user_type = 'Shared'GROUP BY garage_idORDER BY num_unique_users DESC\n\n\n\n\n\n\n\n\ngarage_id\nnum_unique_users\n\n\n\n\n0\nBl2\n18\n\n\n1\nAsO2\n17\n\n\n2\nUT9\n16\n\n\n3\nAdO3\n3\n\n\n4\nMS1\n2\n\n\n5\nSR2\n2\n\n\n6\nAdA1\n1\n\n\n7\nRis\n1\n\n\n\n\n\n\n\n\n-- most_popular_shared_start_timesSELECT weekdays_plugin, start_plugin_hour, Count(*) as num_charging_sessionsFROM charging_sessionsWHERE user_type = 'Shared'GROUP BY weekdays_plugin, start_plugin_hourORDER BY num_charging_sessions DESCLIMIT 10\n\n\n\n\n\n\n\n\nweekdays_plugin\nstart_plugin_hour\nnum_charging_sessions\n\n\n\n\n0\nSunday\n17\n30\n\n\n1\nFriday\n15\n28\n\n\n2\nThursday\n19\n26\n\n\n3\nThursday\n16\n26\n\n\n4\nWednesday\n19\n25\n\n\n5\nSunday\n18\n25\n\n\n6\nSunday\n15\n25\n\n\n7\nMonday\n15\n24\n\n\n8\nFriday\n16\n24\n\n\n9\nTuesday\n16\n23\n\n\n\n\n\n\n\n\n-- long_duration_shared_usersSELECT * FROM( SELECT user_id, AVG(duration_hours) as avg_charging_duration    FROM public.charging_sessions   WHERE user_type = 'Shared'  GROUP BY public.charging_sessions.user_id   ORDER BY avg_charging_duration DESC ) as calcWHERE calc.avg_charging_duration is not null AND calc.avg_charging_duration &gt; 10\n\n\n\n\n\n\n\n\nuser_id\navg_charging_duration\n\n\n\n\n0\nShare-9\n16.845833\n\n\n1\nShare-17\n12.894556\n\n\n2\nShare-25\n12.214475\n\n\n3\nShare-18\n12.088807\n\n\n4\nShare-8\n11.550431\n\n\n5\nAdO3-1\n10.369387"
  },
  {
    "objectID": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html",
    "href": "datacamp_projects/workspace/SQL/student_mental_health/notebook.html",
    "title": "Heiner Atze, PhD",
    "section": "",
    "text": "Illustration of silhouetted heads\n\n\nDoes going to university in a different country affect your mental health? A Japanese international university surveyed its students in 2018 and published a study the following year that was approved by several ethical and regulatory boards.The study found that international students have a higher risk of mental health difficulties than the general population, and that social connectedness (belonging to a social group) and acculturative stress (stress associated with joining a new culture) are predictive of depression.Explore the students data using PostgreSQL to find out if you would come to a similar conclusion for international students and see if the length of stay is a contributing factor.Here is a data description of the columns you may find helpful.\nField Name | Description || ————- | ———————————————— || inter_dom | Types of students (international or domestic) || japanese_cate | Japanese language proficiency || english_cate | English language proficiency || academic | Current academic level (undergraduate or graduate) || age | Current age of student || stay | Current length of stay in years || todep | Total score of depression (PHQ-9 test) || tosc | Total score of social connectedness (SCS test) || toas | Total score of acculturative stress (ASISS test) |\n\n-- Run this code to view the data in studentsSELECT * FROM students;\n\n\n\n\n\n\n\n\ninter_dom\nregion\ngender\nacademic\nage\nage_cate\nstay\nstay_cate\njapanese\njapanese_cate\nenglish\nenglish_cate\nintimate\nreligion\nsuicide\ndep\ndeptype\ntodep\ndepsev\ntosc\napd\nahome\naph\nafear\nacs\naguilt\namiscell\ntoas\npartner\nfriends\nparents\nrelative\nprofess\nphone\ndoctor\nreli\nalone\nothers\ninternet\npartner_bi\nfriends_bi\nparents_bi\nrelative_bi\nprofessional_bi\nphone_bi\ndoctor_bi\nreligion_bi\nalone_bi\nothers_bi\ninternet_bi\n\n\n\n\n0\nInter\nSEA\nMale\nGrad\n24.0\n4.0\n5.0\nLong\n3.0\nAverage\n5.0\nHigh\n\nYes\nNo\nNo\nNo\n0.0\nMin\n34.0\n23.0\n9.0\n11.0\n8.0\n11.0\n2.0\n27.0\n91.0\n5.0\n5.0\n6.0\n3.0\n2.0\n1.0\n4.0\n1.0\n3.0\n4.0\nNaN\nYes\nYes\nYes\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n1\nInter\nSEA\nMale\nGrad\n28.0\n5.0\n1.0\nShort\n4.0\nHigh\n4.0\nHigh\n\nNo\nNo\nNo\nNo\n2.0\nMin\n48.0\n8.0\n7.0\n5.0\n4.0\n3.0\n2.0\n10.0\n39.0\n7.0\n7.0\n7.0\n4.0\n4.0\n4.0\n4.0\n1.0\n1.0\n1.0\nNaN\nYes\nYes\nYes\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n2\nInter\nSEA\nMale\nGrad\n25.0\n4.0\n6.0\nLong\n4.0\nHigh\n4.0\nHigh\nYes\nYes\nNo\nNo\nNo\n2.0\nMin\n41.0\n13.0\n4.0\n7.0\n6.0\n4.0\n3.0\n14.0\n51.0\n3.0\n3.0\n3.0\n1.0\n1.0\n2.0\n1.0\n1.0\n1.0\n1.0\nNaN\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n3\nInter\nEA\nFemale\nGrad\n29.0\n5.0\n1.0\nShort\n2.0\nLow\n3.0\nAverage\nNo\nNo\nNo\nNo\nNo\n3.0\nMin\n37.0\n16.0\n10.0\n10.0\n8.0\n6.0\n4.0\n21.0\n75.0\n5.0\n5.0\n5.0\n5.0\n5.0\n2.0\n2.0\n2.0\n4.0\n4.0\nNaN\nYes\nYes\nYes\nYes\nYes\nNo\nNo\nNo\nNo\nNo\nNo\n\n\n4\nInter\nEA\nFemale\nGrad\n28.0\n5.0\n1.0\nShort\n1.0\nLow\n3.0\nAverage\nYes\nNo\nNo\nNo\nNo\n3.0\nMin\n37.0\n15.0\n12.0\n5.0\n8.0\n7.0\n4.0\n31.0\n82.0\n5.0\n5.0\n5.0\n2.0\n5.0\n2.0\n5.0\n5.0\n4.0\n4.0\nNaN\nYes\nYes\nYes\nNo\nYes\nNo\nYes\nYes\nNo\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n281\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n128\n140\n\n\n\n\n\n\n\n\n\n\n\n282\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n137\n131\n\n\n\n\n\n\n\n\n\n\n\n283\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n66\n202\n\n\n\n\n\n\n\n\n\n\n\n284\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n61\n207\n\n\n\n\n\n\n\n\n\n\n\n285\n\n\n\n\nNaN\nNaN\nNaN\n\nNaN\n\nNaN\n\n\n\n\n\n\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n30\n238\n\n\n\n\n\n\n\n\n\n\n\n\n\n286 rows × 50 columns\n\n\n\n\n-- querySELECT  stay,   COUNT(inter_dom) AS count_int,  ROUND(AVG(todep),2) AS average_phq, ROUND(AVG(tosc),2) AS average_scs,  ROUND(AVG(toas),2) AS average_asFROM studentsWHERE inter_dom = 'Inter'GROUP BY stayORDER BY stay DESC\n\n\n\n\n\n\n\n\nstay\ncount_int\naverage_phq\naverage_scs\naverage_as\n\n\n\n\n0\n10\n1\n13.00\n32.00\n50.00\n\n\n1\n8\n1\n10.00\n44.00\n65.00\n\n\n2\n7\n1\n4.00\n48.00\n45.00\n\n\n3\n6\n3\n6.00\n38.00\n58.67\n\n\n4\n5\n1\n0.00\n34.00\n91.00\n\n\n5\n4\n14\n8.57\n33.93\n87.71\n\n\n6\n3\n46\n9.09\n37.13\n78.00\n\n\n7\n2\n39\n8.28\n37.08\n77.67\n\n\n8\n1\n95\n7.48\n38.11\n72.80"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "",
    "text": "So you think you can classify text? How about tweets? In this notebook, we’ll take a dive into the world of social media text classification by investigating how to properly classify tweets from two prominent North American politicians: Donald Trump and Justin Trudeau.\n\n\n\n\n\nPhoto Credit: Executive Office of the President of the United States\n\n\nTweets pose specific problems to NLP, including the fact they are shorter texts. There are also plenty of platform-specific conventions to give you hassles: mentions, #hashtags, emoji, links and short-hand phrases (ikr?). Can we overcome those challenges and build a useful classifier for these two tweeters? Yes! Let’s get started.\n\n\nTo begin, we will import all the tools we need from scikit-learn. We will need to properly vectorize our data (CountVectorizer and TfidfVectorizer). And we will also want to import some models, including MultinomialNB from the naive_bayes module, LinearSVC from the svm module and PassiveAggressiveClassifier from the linear_model module. Finally, we’ll need sklearn.metrics and train_test_split and GridSearchCV from the model_selection module to evaluate and optimize our model.\n\n\n# Set seed for reproducibility\nimport random; random.seed(53)\n\n# Import all we need from sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introduction-and-imports",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introduction-and-imports",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "",
    "text": "So you think you can classify text? How about tweets? In this notebook, we’ll take a dive into the world of social media text classification by investigating how to properly classify tweets from two prominent North American politicians: Donald Trump and Justin Trudeau.\n\n\n\n\n\nPhoto Credit: Executive Office of the President of the United States\n\n\nTweets pose specific problems to NLP, including the fact they are shorter texts. There are also plenty of platform-specific conventions to give you hassles: mentions, #hashtags, emoji, links and short-hand phrases (ikr?). Can we overcome those challenges and build a useful classifier for these two tweeters? Yes! Let’s get started.\n\n\nTo begin, we will import all the tools we need from scikit-learn. We will need to properly vectorize our data (CountVectorizer and TfidfVectorizer). And we will also want to import some models, including MultinomialNB from the naive_bayes module, LinearSVC from the svm module and PassiveAggressiveClassifier from the linear_model module. Finally, we’ll need sklearn.metrics and train_test_split and GridSearchCV from the model_selection module to evaluate and optimize our model.\n\n\n# Set seed for reproducibility\nimport random; random.seed(53)\n\n# Import all we need from sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#transforming-our-collected-data",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#transforming-our-collected-data",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "2. Transforming our collected data",
    "text": "2. Transforming our collected data\n\nTo begin, let’s start with a corpus of tweets which were collected in November 2017. They are available in CSV format. We’ll use a Pandas DataFrame to help import the data and pass it to scikit-learn for further processing.\n\n\nSince the data has been collected via the Twitter API and not split into test and training sets, we’ll need to do this. Let’s use train_test_split() with random_state=53 and a test size of 0.33, just as we did in the DataCamp course. This will ensure we have enough test data and we’ll get the same results no matter where or when we run this code.\n\n\nimport pandas as pd\n\n# Load data\ntweet_df = pd.read_csv('datasets/tweets.csv')\n\n# Create target\ny = tweet_df['author']\nX = tweet_df.drop('author', axis = 1)\n\n# Split training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 53, test_size = 0.33)"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#vectorize-the-tweets",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#vectorize-the-tweets",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "3. Vectorize the tweets",
    "text": "3. Vectorize the tweets\n\nWe have the training and testing data all set up, but we need to create vectorized representations of the tweets in order to apply machine learning.\n\n\nTo do so, we will utilize the CountVectorizer and TfidfVectorizer classes which we will first need to fit to the data.\n\n\nOnce this is complete, we can start modeling with the new vectorized tweets!\n\n\n# Initialize count vectorizer\ncount_vectorizer = CountVectorizer(max_df = 0.9, min_df = 0.05, stop_words = 'english')\ncount_vectorizer.fit(X_train['status'])\n\n# Create count train and test variables\ncount_train = count_vectorizer.transform(X_train['status'])\ncount_test = count_vectorizer.transform(X_test['status'])\n\n# Initialize tfidf vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.9, min_df = 0.05)\\\n                    .fit(X_train['status'])\n\n# Create tfidf train and test variables\ntfidf_train = tfidf_vectorizer.transform(X_train['status'])\ntfidf_test = tfidf_vectorizer.transform(X_test['status'])"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#training-a-multinomial-naive-bayes-model",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#training-a-multinomial-naive-bayes-model",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "4. Training a multinomial naive Bayes model",
    "text": "4. Training a multinomial naive Bayes model\n\nNow that we have the data in vectorized form, we can train the first model. Investigate using the Multinomial Naive Bayes model with both the CountVectorizer and TfidfVectorizer data. Which do will perform better? How come?\n\n\nTo assess the accuracies, we will print the test sets accuracy scores for both models.\n\n\n# Create a MulitnomialNB model\ntfidf_nb = MultinomialNB().fit(tfidf_train, y_train)\n\n# ... Train your model here ...\n\n# Run predict on your TF-IDF test data to get your predictions\ntfidf_nb_pred = tfidf_nb.predict(tfidf_test)\n\n# Calculate the accuracy of your predictions\ntfidf_nb_score = metrics.accuracy_score(y_test, tfidf_nb_pred)\n\n# Create a MulitnomialNB model\ncount_nb = MultinomialNB().fit(count_train, y_train)\n# ... Train your model here ...\n\n# Run predict on your count test data to get your predictions\ncount_nb_pred = count_nb.predict(count_test)\n\n# Calculate the accuracy of your predictions\ncount_nb_score = metrics.accuracy_score(y_test, count_nb_pred)\n\nprint('NaiveBayes Tfidf Score: ', tfidf_nb_score)\nprint('NaiveBayes Count Score: ', count_nb_score)\n\nNaiveBayes Tfidf Score:  0.803030303030303\nNaiveBayes Count Score:  0.7954545454545454"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#evaluating-our-model-using-a-confusion-matrix",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#evaluating-our-model-using-a-confusion-matrix",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "5. Evaluating our model using a confusion matrix",
    "text": "5. Evaluating our model using a confusion matrix\n\nWe see that the TF-IDF model performs better than the count-based approach. Based on what we know from the NLP fundamentals course, why might that be? We know that TF-IDF allows unique tokens to have a greater weight - perhaps tweeters are using specific important words that identify them! Let’s continue the investigation.\n\n\nFor classification tasks, an accuracy score doesn’t tell the whole picture. A better evaluation can be made if we look at the confusion matrix, which shows the number correct and incorrect classifications based on each class. We can use the metrics, True Positives, False Positives, False Negatives, and True Negatives, to determine how well the model performed on a given class. How many times was Trump misclassified as Trudeau?\n\n\n%matplotlib inline\n\nfrom datasets.helper_functions import plot_confusion_matrix\n\n# Calculate the confusion matrices for the tfidf_nb model and count_nb models\ntfidf_nb_cm = metrics.confusion_matrix(y_test, tfidf_nb_pred)\ncount_nb_cm = metrics.confusion_matrix(y_test, count_nb_pred)\n\n# Plot the tfidf_nb_cm confusion matrix\nplot_confusion_matrix(tfidf_nb_cm, classes=['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"TF-IDF NB Confusion Matrix\")\n\n# Plot the count_nb_cm confusion matrix without overwriting the first plot \nplot_confusion_matrix(count_nb_cm, classes=['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"Count NB Confusion matrix\", figure=1)\n\nConfusion matrix, without normalization\nConfusion matrix, without normalization"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#trying-out-another-classifier-linear-svc",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#trying-out-another-classifier-linear-svc",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "6. Trying out another classifier: Linear SVC",
    "text": "6. Trying out another classifier: Linear SVC\n\nSo the Bayesian model only has one prediction difference between the TF-IDF and count vectorizers – fairly impressive! Interestingly, there is some confusion when the predicted label is Trump but the actual tweeter is Trudeau. If we were going to use this model, we would want to investigate what tokens are causing the confusion in order to improve the model.\n\n\nNow that we’ve seen what the Bayesian model can do, how about trying a different approach? LinearSVC is another popular choice for text classification. Let’s see if using it with the TF-IDF vectors improves the accuracy of the classifier!\n\n\n# Create a LinearSVM model\ntfidf_svc = LinearSVC().fit(tfidf_train, y_train)\n\n# ... Train your model here ...\n\n# Run predict on your tfidf test data to get your predictions\ntfidf_svc_pred = tfidf_svc.predict(tfidf_test)\n\n# Calculate your accuracy using the metrics module\ntfidf_svc_score = metrics.accuracy_score(y_test, tfidf_svc_pred)\n\nprint(\"LinearSVC Score:   %0.3f\" % tfidf_svc_score)\n\n# Calculate the confusion matrices for the tfidf_svc model\nsvc_cm = metrics.confusion_matrix(y_test, tfidf_svc_pred)\n\n# Plot the confusion matrix using the plot_confusion_matrix function\nplot_confusion_matrix(svc_cm, classes = ['Donald J. Trump', 'Justin Trudeau'],\n                      title=\"TF-IDF LinearSVC Confusion Matrix\")\n\nLinearSVC Score:   0.841\nConfusion matrix, without normalization"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introspecting-our-top-model",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#introspecting-our-top-model",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "7. Introspecting our top model",
    "text": "7. Introspecting our top model\n\nWow, the LinearSVC model is even better than the Multinomial Bayesian one. Nice work! Via the confusion matrix we can see that, although there is still some confusion where Trudeau’s tweets are classified as Trump’s, the False Positive rate is better than the previous model. So, we have a performant model, right?\n\n\nWe might be able to continue tweaking and improving all of the previous models by learning more about parameter optimization or applying some better preprocessing of the tweets.\n\n\nNow let’s see what the model has learned. Using the LinearSVC Classifier with two classes (Trump and Trudeau) we can sort the features (tokens), by their weight and see the most important tokens for both Trump and Trudeau. What are the most Trump-like or Trudeau-like words? Did the model learn something useful to distinguish between these two men?\n\n\nfrom datasets.helper_functions import plot_and_return_top_features\n\n# Import pprint from pprint\nfrom pprint import pprint\n\n# Get the top features using the plot_and_return_top_features function and your top model and tfidf vectorizer\ntop_features = plot_and_return_top_features(tfidf_svc, tfidf_vectorizer)\n\n# pprint the top features\npprint(top_features)\n\n\n\n\n\n\n\n\n[(-0.3959834966911922, 'great'),\n (-0.24645580091925237, 'thank'),\n (0.06257998949180026, 'president'),\n (0.48211745246750215, 'https'),\n (0.5960555762649068, 'vietnam'),\n (0.6155609686456073, 'amp'),\n (0.7725857577713344, 'le'),\n (0.8213735137856691, 'les'),\n (0.8286549508433744, 'today'),\n (1.1869092357816051, 'du'),\n (1.3143518952322126, 'pour'),\n (1.4122560793508427, 'nous'),\n (1.4612710235935042, 'rt'),\n (1.4991808273544363, 'et'),\n (1.50564270245237, 'la'),\n (1.6567934485943738, 'canada')]"
  },
  {
    "objectID": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#bonus-can-you-write-a-trump-or-trudeau-tweet",
    "href": "datacamp_projects/guided/python/trump_trudeau_tweets/trump_trudeau_tweets.html#bonus-can-you-write-a-trump-or-trudeau-tweet",
    "title": "Trump vs. Trudeau: Tweet classification",
    "section": "8. Bonus: can you write a Trump or Trudeau tweet?",
    "text": "8. Bonus: can you write a Trump or Trudeau tweet?\n\nSo, what did our model learn? It seems like it learned that Trudeau tweets in French!\n\n\nI challenge you to write your own tweet using the knowledge gained to trick the model! Use the printed list or plot above to make some inferences about what words will classify your text as Trump or Trudeau. Can you fool the model into thinking you are Trump or Trudeau?\n\n\nIf you can write French, feel free to make your Trudeau-impersonation tweet in French! As you may have noticed, these French words are common words, or, “stop words”. You could remove both English and French stop words from the tweets as a preprocessing step, but that might decrease the accuracy of the model because Trudeau is the only French-speaker in the group. If you had a dataset with more than one French speaker, this would be a useful preprocessing step.\n\n\nFuture work on this dataset could involve:\n\n\n\nAdd extra preprocessing (such as removing URLs or French stop words) and see the effects\n\n\nUse GridSearchCV to improve both your Bayesian and LinearSVC models by finding the optimal parameters\n\n\nIntrospect your Bayesian model to determine what words are more Trump- or Trudeau- like\n\n\nAdd more recent tweets to your dataset using tweepy and retrain\n\n\n\nGood luck writing your impersonation tweets – feel free to share them on Twitter!\n\n\n# Write two tweets as strings, one which you want to classify as Trump and one as Trudeau\ntrump_tweet = \"Covfeve\"\ntrudeau_tweet = \"Make Canada great again!\"\n\n# Vectorize each tweet using the TF-IDF vectorizer's transform method\n# Note: `transform` needs the string in a list object (i.e. [trump_tweet])\ntrump_tweet_vectorized = tfidf_vectorizer.transform([trump_tweet])\ntrudeau_tweet_vectorized = tfidf_vectorizer.transform([trudeau_tweet])\n\n# Call the predict method on your vectorized tweets\ntrump_tweet_pred = tfidf_svc.predict(trump_tweet_vectorized)\ntrudeau_tweet_pred = tfidf_svc.predict(trudeau_tweet_vectorized)\n\nprint(\"Predicted Trump tweet\", trump_tweet_pred)\nprint(\"Predicted Trudeau tweet\", trudeau_tweet_pred)\n\nPredicted Trump tweet ['Donald J. Trump']\nPredicted Trudeau tweet ['Justin Trudeau']"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Heiner Atze, PhD, Pharmacist",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\njdaskölfs",
    "crumbs": [
      "Home"
    ]
  }
]